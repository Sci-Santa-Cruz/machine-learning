{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron\">\n",
    "  <h1><i class=\"fa fa-bar-chart\" aria-hidden=\"true\"></i> Validación, overfitting y métricas de rendimiento  </h1>\n",
    "  <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ningún método domina a todos los demás sobre todos los conjuntos de datos posibles. En un conjunto de datos en particular, un método específico puede funcionar mejor, pero algún otro método puede funcionar mejor en un conjunto de datos similar pero diferente. Por lo tanto, es una tarea importante decidir para cualquier conjunto de datos determinado qué método produce los mejores resultados. Seleccionar el mejor enfoque puede ser una de las partes más difíciles de realizar el aprendizaje automático en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/12.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La única forma de saber qué tan bien se generalizará un modelo a los casos nuevos es probarlo en casos nuevos. Una forma de hacerlo es poner su __modelo en producción y monitorear qué tan bien funciona__ . Esto funciona bien, pero si su modelo es terriblemente malo, sus usuarios se quejarán, no es la mejor idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\"Machine Learning Is No Place To “Move Fast And Break Things”__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mejor opción es dividir sus datos en dos conjuntos: el conjunto de entrenamiento y el conjunto de prueba . Como estos nombres implican, usted entrena su modelo usando el conjunto de entrenamiento, y lo prueba usando el conjunto de prueba. La tasa de erroren casos nuevos se llama error de generalización (o error fuera de la muestra ), y al evaluar su modelo en el conjunto de prueba, obtiene una estimación de este error. Este valor le indica qué tan bien funcionará su modelo en instancias que nunca antes había visto.\n",
    "\n",
    "Si el error de entrenamiento es bajo (es decir, su modelo comete pocos errores en el conjunto de entrenamiento) pero el error de generalización es alto, significa que su modelo está sobreajustando los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train_test_split.svg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar un modelo es bastante simple: sólo use un conjunto de prueba. Pero suponga que está dudando entre dos tipos de modelos (por ejemplo, un modelo lineal y un modelo polinómico): ¿cómo puede decidir entre ellos? Una opción es entrenar a ambos y comparar qué tan bien generalizan usando el conjunto de prueba.\n",
    "\n",
    "Ahora suponga que experimentamos con un __modelo basado en instancias como k-NN__.\n",
    "La pregunta es, __¿cómo elige el valor del hiperparámetro k?__ Una opción es entrenar 100 modelos diferentes usando 100 valores diferentes para este hiperparámetro. Suponga que encuentra el mejor valor de hiperparámetro que produce un modelo con el error de generalización más bajo⁠, digamos, solo un error del 5%. Lanzas este modelo en producción, pero desafortunadamente no funciona tan bien como se esperaba y produce 15% de errores. ¿Que es lo que acaba de suceder? \n",
    "\n",
    "El problema es que midió el error de generalización varias veces en el mismo conjunto de prueba, y adaptó el modelo y los hiperparámetros para producir el mejor modelo para ese conjunto en particular . Esto significa que es poco probable que el modelo  también funcione con datos nuevos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train_test_split_matrix.svg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución común a este problema se llama validación de reserva: simplemente tiene una parte del conjunto de capacitación para evaluar varios modelos candidatos y seleccionar el mejor. El nuevo conjunto extendido se denomina conjunto de validación (o, a veces , conjunto de desarrollo o conjunto de desarrollo) Más específicamente, entrena múltiples modelos con varios hiperparámetros en el conjunto de entrenamiento reducido (es decir, el conjunto de entrenamiento completo menos el conjunto de validación), y selecciona el modelo que funciona mejor en el conjunto de validación. Después de este proceso de validación de reserva, entrena el mejor modelo en el conjunto completo de capacitación (incluido el conjunto de validación), y esto le da el modelo final. Por último, evalúa este modelo final en el conjunto de pruebas para obtener una estimación del error de generalización.Esta solución generalmente funciona bastante bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, si el conjunto de validación es __demasiado pequeño__, las evaluaciones del modelo serán imprecisas: puede terminar seleccionando un modelo subóptimo por error. Por el contrario, si el conjunto de validación es __demasiado grande__, el conjunto de entrenamiento restante será mucho más pequeño que el conjunto de entrenamiento completo. ¿Por qué es esto malo? Bueno, dado que el modelo final se entrenará en el conjunto de entrenamiento completo, no es ideal comparar modelos candidatos entrenados en un conjunto de entrenamiento mucho más pequeño. Sería como seleccionar el velocista más rápido para participar en una maratón. \n",
    "\n",
    "\n",
    "<img src=\"images/validation.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "La forma de resolver este problema es realizar una __validación cruzada__ repetida , utilizando muchos conjuntos de validación pequeños. Cada modelo se evalúa una vez por conjunto de validación después de haber sido entrenado en el resto de los datos. Al promediar todas las evaluaciones de un modelo, obtienes una medida mucho más precisa de su rendimiento. Sin embargo, hay un inconveniente: el tiempo de entrenamiento se multiplica por el número de conjuntos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/crossval.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>remuestreo</b>:\n",
    "    \n",
    "    \n",
    "Los métodos de remuestreo son una herramienta indispensable en las estadísticas modernas. Implican extraer muestras repetidamente de un conjunto de entrenamiento y volver a colocar un modelo de interés en cada muestra para obtener información adicional sobre el modelo ajustado. Por ejemplo, para estimar la variabilidad de un ajuste de regresión lineal, podemos extraer repetidamente diferentes muestras de los datos de entrenamiento, ajustar una regresión lineal a cada nueva muestra y luego examinar en qué medida difieren los ajustes resultantes. Tal enfoque puede permitirnos obtener información que no estaría disponible al ajustar el modelo solo una vez usando la muestra de entrenamiento original.\n",
    "Los enfoques de remuestreo pueden ser computacionalmente caros, porque implican ajustar el mismo método estadístico varias veces utilizando diferentes subconjuntos de datos de entrenamiento. Sin embargo, debido a los recientes avances en potencia informática, los requisitos computacionales de los métodos de remuestreo generalmente no son prohibitivos. En este capítulo, discutimos dos de los métodos de remuestreo más comúnmente utilizados, la validación cruzada y el bootstrap. Ambos métodos son herramientas importantes en la aplicación práctica de muchos procedimientos de aprendizaje estadístico. Por ejemplo, la validación cruzada se puede utilizar para estimar el error de prueba asociado con un método de aprendizaje estadístico determinado para valorar su rendimiento o para seleccionar el nivel apropiado de flexibilidad. El proceso de evaluar el desempeño de un modelo se conoce como evaluación del modelo, mientras que el proceso de seleccionar el nivel adecuado de flexibilidad para un modelo se conoce como selección del modelo de evaluación. El bootstrap se usa en varios contextos, el modelo más común para proporcionar una medida de precisión de una estimación de parámetros o de un método de aprendizaje estadístico de selección dado.\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Existen varias técnicas para evaluar que tan robusto es un modelo, algunas de ellas son: criterio de información de Akaike [7], estadística $C_p>$ de Mallows [8], remuestreo [9], jackknife [10], validación simple [1,2], validación cruzada [3]. El general cualquier técnica de evaluación de la tasa de generalización de modelos de clasificación persigue dos objetivos: (a) selección del modelo (b) y estimación del rendimiento del modelo seleccionado [19]. __Se espera que el rendimiento final del modelo tenga un sesgo (error) y variación bajos__.\n",
    "\n",
    "La técnica más popular y aceptada para la estimación del rendimiento y la selección del modelo es la de validación cruzada [24,25]. Existen diferentes enfoques de validación cruzada como son: __k- validación cruzada (k-VC) [3], validación cruzada dejando uno fuera (caso especial de k-VC), validación cruzada Monte Carlo (VCMC)[18], validación cruzada con entrenamiento-validación-prueba (VCEVP) [23], validación cruzada anidada (VCA)[27] (la cual es una generalización de las técnicas de validación con protocolo entrenamineto validación-prueba), método de Tibshirani y Tibshirani [26], validación cruzada con movimiento de ventanas [28] y repetir la doble validación cruzada [29] (usualmente más de 100 veces)__. Para dichos enfoques se le puede agregar la opción de __estratificar__ las particiones o tomar las particiones __sin estratificación__ . La VCA se puede combinar con alguna otra técnica como por ejemplo: remuestreo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/14.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <H3>TIP</H3>\n",
    "    \n",
    "    \n",
    "It is common to use 80% of the data for training and hold out 20% for testing. However, this depends on the size of the dataset: if it contains 10 million instances, then holding out 1% means your test set will contain 100,000 instances, probably more than enough to get a good estimate of the generalization error.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <H3>No Free Lunch Theorem</H3>\n",
    "\n",
    "Un modelo es una versión simplificada de las observaciones. Las simplificaciones están destinadas a descartar los detalles superfluos que es poco probable que se generalicen a nuevas instancias. Para decidir qué datos desechar y qué datos conservar, debe hacer suposiciones . \n",
    "\n",
    "Por ejemplo, un modelo lineal supone que los datos son fundamentalmente lineales y que la distancia entre las instancias y la línea recta es solo ruido, que puede ignorarse de manera segura.En un famoso artículo de 1996 , 11 David Wolpertdemostró que si no hace ninguna suposición sobre los datos, entonces no hay razón para preferir un modelo sobre otro. Esto se llama el teorema de No Free Lunch (NFL). Para algunos conjuntos de datos, el mejor modelo es un modelo lineal, mientras que para otros conjuntos de datos es una red neuronal. No existe un modelo que a priori garantice que funcione mejor (de ahí el nombre del teorema).\n",
    "\n",
    "La única forma de saber con certeza qué modelo es el mejor es evaluarlos a todos. Como esto no es posible, en la práctica hace algunas suposiciones razonables sobre los datos y evalúa solo unos pocos modelos razonables. Por ejemplo, para tareas simples, puede evaluar modelos lineales con varios niveles de regularización, y para un problema complejo, puede evaluar varias redes neuronales.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales desafíos del aprendizaje automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobre ajuste \n",
    "\n",
    "En resumen, dado que su tarea principal es seleccionar un algoritmo de aprendizaje y entrenarlo en algunos datos, las dos cosas que pueden salir mal son \"algoritmo incorrecto\" y \"datos incorrectos\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "\n",
    "EJEMPLOS DE SESGO DE MUESTREO\n",
    "\n",
    "Quizás el ejemplo más famoso de sesgo de muestreo ocurrió durante las elecciones presidenciales de Estados Unidos en 1936, que enfrentó a Landon contra Roosevelt: el Literary Digest realizó una encuesta muy grande, enviando correos a unos 10 millones de personas. Obtuvo 2,4 millones de respuestas y predijo con gran confianza que Landon obtendría el 57% de los votos. En cambio, Roosevelt ganó con el 62% de los votos. La falla estaba en el método de muestreo del Literary Digest :\n",
    "\n",
    "Primero, para obtener las direcciones a las que enviar las encuestas, el Literary Digest utilizó directorios telefónicos, listas de suscriptores de revistas, listas de miembros del club y similares. Todas estas listas tendían a favorecer a las personas más ricas, que tenían más probabilidades de votar por los republicanos (de ahí Landon).\n",
    "\n",
    "Segundo, menos del 25% de las personas encuestadas respondieron. Nuevamente, esto introdujo un sesgo de muestreo, al descartar potencialmente a las personas a las que no les importaba mucho la política, a las personas que no les gustaba el Literary Digest y otros grupos clave. Este es un tipo especial de sesgo de muestreo llamado sesgo de no respuesta .\n",
    "\n",
    "Aquí hay otro ejemplo: digamos que desea construir un sistema para reconocer videos de música funk. Una forma de construir su conjunto de entrenamiento es buscar \"música funk\" en YouTube y usar los videos resultantes. Pero esto supone que el motor de búsqueda de YouTube devuelve un conjunto de videos que son representativos de todos los videos de música funk en YouTube. En realidad, es probable que los resultados de la búsqueda estén sesgados hacia artistas populares (y si vives en Brasil obtendrás muchos videos de \"funk carioca\", que no se parecen en nada a James Brown). Por otro lado, ¿de qué otra forma puedes obtener un gran conjunto de entrenamiento?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "__Sobreajustar y ajuste insuficiente de los datos de entrenamiento__ \n",
    "\n",
    "Estás visitando un país extranjero y el taxista te estafa. Puede sentirse tentado a decir que todos los taxistas en ese país son ladrones. El __exceso de generalización__ es algo que los humanos hacemos con demasiada frecuencia, y desafortunadamente las máquinas pueden caer en la misma trampa si no tenemos cuidado. En Machine Learning, esto se denomina sobreajuste : significa que el modelo funciona bien en los datos de entrenamiento, pero no se generaliza bien.La figura  muestra un ejemplo de un modelo de satisfacción con la vida polinómica de alto grado que se adapta fuertemente a los datos de entrenamiento. A pesar de que se desempeña mucho mejor en los datos de entrenamiento que el modelo lineal simple, ¿realmente confiaría en sus predicciones?\n",
    "\n",
    "\n",
    "<img src=\"images/mls2_0122.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "Los modelos complejos, como las redes neuronales profundas, pueden detectar patrones sutiles en los datos, pero si el conjunto de entrenamiento es ruidoso o si es demasiado pequeño (lo que introduce ruido de muestreo), es probable que el modelo detecte patrones en el ruido mismo. Obviamente, estos patrones no se generalizarán a nuevas instancias. Por ejemplo, supongamos que alimenta su modelo de satisfacción con la vida con muchos más atributos, incluidos los no informativos como el nombre del país. En ese caso, un modelo complejo puede detectar patrones como el hecho de que todos los países en los datos de entrenamiento con una u en su nombre tienen una satisfacción con la vida mayor que 7: Nueva Zelanda (7.3), Noruega (7.4), Suecia (7.2), y Suiza (7.5). ¿Qué tan seguro estás de que el u-la regla de satisfacción generaliza a Ruanda  o Zimbabue? Obviamente, este patrón ocurrió en los datos de entrenamiento por pura casualidad, pero el modelo no tiene forma de saber si un patrón es real o simplemente el resultado del ruido en los datos.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos dos conjuntos de datos modelados por una regresión lineal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sobre_a_2.png\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la segunda gráfica los datos tiene un mayor error , ya que la recta no se adapta a los datos de entrenamiento. Sabemos que si encrementamos el grado de la variable del modelo se incrementa la flexibilidad del mismo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sobre_a_3.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con un grado igual a 3 los datos de entrenamiento se acoplan mejor al modelo y los datos de prueba actuán de la misma forma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sobre_a_4.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si comporamos con el modelo lineal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sobre_a_5.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿pero que pasa si incrementmos el grado del polinomio que modela los datos ? hay un mejor ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sobre_a_6.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pero en los datos de prueba hay un mayor error..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sobre_a_7.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gráficamente esto es un sub ajuste y un sobre ajuste del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sobre_a_8.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el sobre ajuste puede ser por sesgo o por varianza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "Advertencia\n",
    "    \n",
    "    \n",
    "El sobreajuste ocurre cuando el modelo es demasiado complejo en relación con la cantidad y el ruido de los datos de entrenamiento. Aquí hay posibles soluciones:\n",
    "\n",
    "- Simplifique el modelo seleccionando uno con menos parámetros (p. Ej., Un modelo lineal en lugar de un modelo polinomial de alto grado), reduciendo el número de atributos en los datos de entrenamiento o restringiendo el modelo.\n",
    "\n",
    "- Recopila más datos de entrenamiento.\n",
    "    \n",
    "- Reduzca el ruido en los datos de entrenamiento (por ejemplo, corrija los errores de datos y elimine los valores atípicos).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Comose puede adivinar, la adaptación insuficiente es lo contrario de la adaptación excesiva : ocurre cuando su modelo es demasiado simple para aprender la estructura subyacente de los datos. Por ejemplo, un modelo lineal de satisfacción con la vida es propenso a la falta de ajuste; la realidad es simplemente más compleja que el modelo, por lo que sus predicciones seguramente serán inexactas, incluso en los ejemplos de capacitación .Estas son las principales opciones para solucionar este problema:Seleccione un modelo más potente, con más parámetros.Alimente mejores funciones al algoritmo de aprendizaje (ingeniería de características).Reduzca las restricciones en el modelo (por ejemplo, reduzca el hiperparámetro de regularización)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en la regresión, el sobre ajuste o sub ajuste es un problema de la clasificación y el clustering. Saber identificarlos es lo que separa a los amaters de los profesionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agregar imágenes  de overfitting para clasificación ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Mismatch__\n",
    "\n",
    "\n",
    "En algunos casos, es fácil obtener una gran cantidad de datos para el entrenamiento, pero estos datos probablemente no serán perfectamente representativos de los datos que se utilizarán en la producción. \n",
    "\n",
    "Por ejemplo, suponga que desea crear una aplicación móvil para tomar fotos de flores y determinar automáticamente sus especies. Puede descargar fácilmente millones de imágenes de flores en la web, pero no serán perfectamente representativas de las imágenes que realmente se tomarán con la aplicación en un dispositivo móvil. Quizás solo tenga 10,000 imágenes representativas (es decir, tomadas con la aplicación). En este caso, __la regla más importante para recordar es que el conjunto de validación y el conjunto de prueba deben ser lo más representativos posible de los datos que espera utilizar en la producción__ , por lo que deben estar compuestos exclusivamente por imágenes representativas: puede mezclarlos y poner la mitad en el conjunto de validación y la otra mitad en el conjunto de prueba (asegurándose de que no haya duplicados o casi duplicados en ambos conjuntos). \n",
    "\n",
    "Pero después de entrenar a su modelo en las imágenes web, si observa que el rendimiento del modelo en el conjunto de validación es decepcionante, no sabrá si esto se debe a que su modelo ha sobreajustado el conjunto de entrenamiento, o si esto se debe solo a desajuste entre las imágenes web y las imágenes de la aplicación móvil o si esto se debe solo a la falta de coincidencia entre las imágenes web y las imágenes de la aplicación móvil.\n",
    "\n",
    "\n",
    "__La efectividad irracional de los datos__\n",
    "\n",
    "\n",
    "En un famoso artículo publicadoen 2001, los investigadores de Microsoft Michele Banko y Eric Brill demostraron que algoritmos de aprendizaje automático muy diferentes, incluidos los bastante simples, funcionaban de manera casi idéntica en un problema complejo de desambiguación del lenguaje natural una vez que recibían suficientes datos:  __La importancia de los datos frente a los algoritmos__\n",
    "\n",
    "<img src=\"images/mls2_0120.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "Como Los autores lo expresaron, __\"estos resultados sugieren que es posible que queramos reconsiderar la compensación entre gastar tiempo y dinero en el desarrollo de algoritmos versus gastarlo en el desarrollo del corpus\"__.La idea de que los datos importan más que los algoritmos para problemas complejos fue más popularizada por Peter Norvig et al. en un artículo titulado __\"La efectividad irracional de los datos\"__ , publicado en 2009. Sin embargo, debe tenerse en cuenta que los conjuntos de datos pequeños y medianos siguen siendo muy comunes, y no siempre es fácil o barato obtener datos adicionales de capacitación. ⁠: así que no abandones los algoritmos todavía."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Datos de entrenamiento no representativos__\n",
    "\n",
    "Para __generalizar bien__ , es crucial que sus datos de capacitación sean representativos de los nuevos casos a los que desea generalizar. Esto es cierto tanto si utiliza el aprendizaje basado en instancias como el aprendizaje basado en modelos.Por ejemplo, el conjunto de países que utilizamos para entrenar el modelo lineal no es perfectamente representativo; faltaban algunos países. La Figura  muestra cómo se ven los datos cuando agrega los países que faltan.\n",
    "\n",
    "Una muestra de entrenamiento más representativa se observa si entrena un modelo lineal sobre estos datos, obtiene la línea continua, mientras que el modelo anterior está representado por la línea de puntos. Como puede ver, la adición de unos pocos países faltantes altera significativamente el modelo, sino que deja en claro que un modelo lineal tan simple probablemente nunca funcionará bien. Parece que los países muy ricos no son más felices que los países moderadamente ricos (de hecho, parecen más infelices) y, por el contrario, algunos países pobres parecen más felices que muchos países ricos. \n",
    "\n",
    "Al usar un conjunto de entrenamiento no representativo, entrenamos un modelo que es poco probable que haga predicciones precisas, especialmente para países muy pobres y muy ricos. Es crucial utilizar un conjunto de capacitación que sea representativo de los casos a los que desea generalizar. Esto suele ser más difícil de lo que parece: _si la muestra es demasiado pequeña,tienen ruido de muestreo (es decir, datos no representativos como resultado de la casualidad), pero incluso muestras muy grandes pueden ser no representativas si el método de muestreo es defectuoso_ . __Esto se llama sesgo de muestreo__ . \n",
    "<img src=\"images/mls2_0121.png\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Datos de baja calidad__\n",
    "\n",
    "Obviamente, si sus datos de entrenamientoestá lleno de errores, valores atípicos y ruido (por ejemplo, debido a mediciones de baja calidad), dificultará que el sistema detecte los patrones subyacentes, por lo que es menos probable que su sistema funcione bien. \n",
    "\n",
    "A menudo vale la pena el esfuerzo de pasar tiempo limpiando sus datos de entrenamiento. La verdad es que la mayoría de los científicos de datos pasan una parte importante de su tiempo haciendo exactamente eso. Los siguientes son algunos ejemplos de cuándo desea limpiar los datos de capacitación:Si algunas instancias son claramente atípicas, puede ser útil simplemente descartarlas o tratar de corregir los errores manualmente.\n",
    "\n",
    "Si a algunas instancias les faltan algunas características (por ejemplo, el 5% de sus clientes no especificaron su edad), debe decidir si desea ignorar este atributo por completo, ignorar estas instancias, completar los valores faltantes (por ejemplo, con la mediana edad), o entrene a un modelo con la función y un modelo sin ella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Características irrelevantes__\n",
    "\n",
    "El dicho dice: basura adentro, basura afuera. Su sistema solo podrá aprender si los datos de entrenamiento contienen suficientes características relevantes y no demasiadas irrelevantes. Una parte crítica del éxito de un proyecto de Machine Learning es crear un buen conjunto de características para entrenar. \n",
    "\n",
    "El proceso, es denominado __ingeniería de características__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dar un paso atrás__\n",
    "\n",
    "\n",
    "Porahora sabes mucho sobre Machine Learning. Sin embargo, pasamos por tantos conceptos que puede que te sientas un poco perdido, así que retrocedamos y veamos el panorama general :Machine Learning se trata de hacer que las máquinas mejoren en alguna tarea al aprender de los datos, en lugar de tener que codificar explícitamente las reglas.Existen muchos tipos diferentes de sistemas ML: supervisados o no, por lotes o en línea, basados ​​en instancias o basados ​​en modelos.En un proyecto de ML, reúne datos en un conjunto de entrenamiento y alimenta el conjunto de entrenamiento a un algoritmo de aprendizaje. \n",
    "\n",
    "Si el algoritmo se basa en el modelo, ajusta algunos parámetros para ajustar el modelo al conjunto de entrenamiento (es decir, para hacer buenas predicciones sobre el conjunto de entrenamiento en sí) y, con suerte, también podrá hacer buenas predicciones en casos nuevos. . Si el algoritmo se basa en instancias, solo aprende los ejemplos de memoria y se generaliza a nuevas instancias mediante el uso de una medida de similitud para compararlas con las instancias aprendidas.El sistema no funcionará bien si su conjunto de entrenamiento es demasiado pequeño o si los datos no son representativos, son ruidosos o están contaminados con características irrelevantes (basura, basura). Por último, su modelo no debe ser demasiado simple (en cuyo caso se ajustará) ni demasiado complejo (en cuyo caso se sobreajustará).Solo hay un último tema importante para cubrir: una vez que haya entrenado un modelo, no quiere simplemente \"esperar\" que se generalice a nuevos casos. Desea evaluarlo y ajustarlo si es necesario. Veamos cómo hacer eso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Métricas de rendimiento\n",
    " \n",
    " #### La optimización del rendimiento\n",
    "El ajuste del rendimiento y la detección de errores son las iteraciones más importantes para un sistema de aprendizaje automático, ya que ayuda a mejorar el rendimiento del sistema. Se considera que los sistemas de aprendizaje automático tienen un rendimiento óptimo si la función generalizada del algoritmo produce un error de generalización bajo con una probabilidad alta. Esto se conoce convencionalmente como __probably approximately correct theory__ (aprendizaje correcto probablemente aproximado, Aprendizaje PAC)\n",
    "\n",
    "\n",
    "Para calcular el error de generalización, que es la precisión de la clasificación o el error en el pronóstico del modelo de regresión, utilizamos las métricas descritas en las siguientes secciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusión \n",
    "\n",
    "La matriz de confusión es una forma tabular de representar el rendimiento de un clasificador. Se puede usar para resumir cómo se comportó el clasificador en el conjunto de prueba, y solo se puede usar en el caso de problemas de clasificación de varias clases. Cada fila de la matriz representa las instancias en una clase predicha, mientras que cada columna representa las instancias en una clase real.\n",
    "\n",
    "\n",
    "<img src=\"images/20.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truco: para recordar fácilmente la matriz de confusión:\n",
    "\n",
    "- Positive o Negative: se refiere a la predicción. Si el modelo predice 1 entonces será positivo, y se predice 0 será negativo.\n",
    "- True o False : se refiere si la predicción es correcta o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cm2.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale la pena señalar que la matriz de confusión no es una métrica ; de hecho, la matriz por sí sola no mide el rendimiento del modelo, sino que es la base para calcular varias métricas útiles, todas ellas basadas en los conceptos de verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos.\n",
    "\n",
    "Todos estos términos se refieren a una  sola clase ; Esto significa que debe considerar un problema de clasificación multiclase como un problema de clasificación binaria al calcular estos términos. Dado un problema de clasificación multiclase, cuyas clases son A, B, ..., Z, tenemos, por ejemplo, lo siguiente:\n",
    "\n",
    "- ( TP ) Verdaderos positivos de A : todas las instancias de A que se clasifican como A cuando realmente son A\n",
    "- ( TN ) Verdaderos negativos de A : todas las instancias no A que no están clasificadas como A cuando realmente son no A\n",
    "- ( FP ) Falsos positivos de A : todas las instancias no A que se clasifican como A pero que son no A\n",
    "- ( FN ) Falsos negativos de A : todas las instancias de A que  están clasificadas como no A pero que son A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/cf.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "La exactitud es una de las métricas populares utilizadas para medir el rendimiento de los modelos ML. La medición es fácil de entender y ayuda al profesional a comunicar la bondad de un modelo muy fácilmente  a sus usuarios comerciales.Generalmente, esta métrica se usa para problemas de clasificación. La precisión se mide como el número de predicciones correctas dividido por el número total de predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/Confusion_Matrix2_2.png\" width=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "Con la métrica de precisión podemos medir la calidad del modelo de machine learning en tareas de clasificación. \n",
    "La precisión es el número de resultados positivos correctos, dividido por el número de resultados positivos predichos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/precision_c2.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nombre de la métrica en sí describe lo que medimos aquí: un número en el rango [0,1] que indica cuán precisas son las predicciones del clasificador: cuanto más alto, mejor. Sin embargo, como en el caso de la precisión, el valor de precisión solo puede ser engañoso. Alta precisión solo significa que, cuando predecimos la clase positiva, somos precisos para detectarla. Pero esto no significa que también somos precisos cuando no estamos detectando esta clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/precision.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall\n",
    "\n",
    "El recuerdo es el número de resultados positivos correctos, dividido por el número de todas las muestras relevantes (por ejemplo, todas las muestras que deben clasificarse como positivas):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/recall_c2.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que la precisión, la recuperación es un número en el rango [0,1] que indica el porcentaje de muestras clasificadas correctamente sobre todas las muestras de esa clase. La recuperación es una métrica importante, especialmente en problemas como la detección de objetos en imágenes.La medición de la precisión y la recuperación de un clasificador binario le permite ajustar el rendimiento del clasificador, haciendo que se comporte según sea necesario.A veces, la precisión es más importante que el recuerdo, y viceversa. Por esta razón, vale la pena dedicar una pequeña sección al régimen clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/recall.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error cuadrático medio\n",
    "El error cuadrático medio ( MSE ) es el promedio de la diferencia cuadrática entre los valores originales y los predichos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mse.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine que para un problema de regresión tenemos la línea de mejor ajuste y queremos medir la distancia de cada punto desde la línea de regresión. El error cuadrático medio  ( MSE ) es la medida estadística que calcularía estas desviaciones. MSE calcula los errores al encontrar la media de los cuadrados para cada una de esas desviaciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que MAE, MSE no tiene límite superior y su límite inferior es 0.\n",
    "\n",
    "Por el contrario, la presencia de los términos cuadrados hace que la métrica sea menos fácil de interpretar. \n",
    "\n",
    "Una buena práctica a seguir es considerar ambas métricas para obtener la mayor información posible sobre la distribución de los errores.\n",
    "\n",
    "La $MAE < MSE $  relación se mantiene, por lo que lo siguiente es cierto:\n",
    "\n",
    "Si $MSE$ está cerca de $MAE$, el regresor comete pequeños errores\n",
    "Si $MSE$ está cerca de $MAE²$, el regresor comete grandes errores\n",
    "\n",
    "Las métricas son probablemente la parte más importante de la selección del modelo ML y las herramientas de medición del rendimiento: expresan las relaciones entre la salida deseada y la salida del modelo. Esta relación es fundamental ya que es para lo que queremos optimizar nuestro modelo. \n",
    "\n",
    "Redes neuronales y aprendizaje profundo ,  donde presentaremos el concepto de la función de pérdida .Además, dado que los modelos que tratamos en este libro son todos modelos paramétricos, podemos medir las métricas durante / al final del proceso de capacitación y guardar los parámetros del modelo (y, por definición, el modelo) que alcanzaron la mejor validación / prueba actuación.\n",
    "\n",
    "El uso de modelos paramétricos nos permite este tipo de flexibilidad:  podemos congelar el estado de un modelo cuando alcanza el rendimiento deseado y continuar con el entrenamiento, cambiar los hiperparámetros y experimentar con diferentes configuraciones / estrategias de entrenamiento, al tiempo que tenemos la certeza de haber almacenado \n",
    "\n",
    "\n",
    "Un modelo que ya tiene un buen rendimiento.Tener métricas y la capacidad de medirlas durante el proceso de capacitación, junto con el uso de modelos paramétricos que se pueden guardar, nos da el poder de evaluar diferentes modelos y guardar solo el que mejor se adapte a nuestras necesidades. Este proceso se llama selección de modelo y es fundamental en cada tubería de ML bien hecha.\n",
    "\n",
    "Nos hemos centrado mucho en el algoritmo de la familia de aprendizaje supervisado, pero, por supuesto, ML es mucho más que esto (incluso los algoritmos de aprendizaje supervisado tienen el mejor rendimiento a la hora de resolver problemas de la vida real).La próxima familia de algoritmos que describiremos brevemente proviene de la familia de aprendizaje no supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error absoluto medio                                                        \n",
    "\n",
    "El error absoluto medio ( MAE ) es otro método estadístico que ayuda a medir la distancia (error) entre dos variables continuas. Una variable continua se puede definir como una variable que podría tener un número infinito de valores cambiantes. Aunque los MAE son difíciles de calcular, se consideran de mejor desempeño que los MSE porque son independientes de la función cuadrada que tiene una mayor influencia en los errores.\n",
    "\n",
    "El error absoluto medio ( MAE ) es el promedio de la diferencia absoluta entre los valores originales y los pronosticados. Como ahora estamos interesados en la medición del rendimiento de un regresor, debemos tener en cuenta que los  valores  y  son valores numéricos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mae.png\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de MAE no tiene límite superior, y su límite inferior es 0. Debería ser evidente que queremos que el valor de MAE sea lo más cercano posible a 0.MAE nos da una indicación de cuán lejos están las predicciones del resultado real; esta métrica es fácilmente interpretable ya que su valor también está en la misma escala que el valor de respuesta original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Régimen clasificador                                                       \n",
    "\n",
    "A veces, puede valer la pena poner un clasificador en el régimen de alta retirada. Esto significa que preferimos tener más falsos positivos y al mismo tiempo estar seguros de que se detectan los verdaderos positivos.\n",
    "\n",
    "El régimen de alta retirada a menudo se requiere en aplicaciones industriales de visión por computadora, donde la línea de producción necesita construir un producto. Luego, al final del proceso de ensamblaje,  un humano controla  si la calidad del producto completo alcanza el estándar requerido.\n",
    "\n",
    "Las aplicaciones de visión por computadora que controlan los robots de ensamblaje generalmente funcionan en un régimen de alta retirada, ya que la línea de producción debe tener un alto rendimiento. Establecer las aplicaciones de visión por computadora en un régimen de alta precisión habría detenido la línea con demasiada frecuencia, reduciendo el rendimiento general y haciendo que la empresa perdiera dinero.\n",
    "\n",
    "La capacidad de cambiar el régimen de trabajo de un clasificador es de extrema importancia en escenarios de la vida real, donde los clasificadores se utilizan como herramientas de producción que deberían adaptarse a las decisiones comerciales.\n",
    "\n",
    "Hay otros casos en los que se requiere un régimen de alta precisión. En escenarios industriales, también hay procesos comandados por aplicaciones de visión por computadora que son críticos y por esta razón, requieren alta precisión.\n",
    "\n",
    "En una línea de producción de motores, los clasificadores podrían usarse para decidir qué parte de la cámara ve la correcta para elegir y ensamblar en el motor. En casos críticos como este, se requiere un régimen de alta precisión y se desaconseja un régimen de alta memoria.Una métrica que combina precisión y recuperación es el puntaje F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puntaje F1\n",
    "\n",
    "La puntuación F1 es la media armónica entre precisión y recuperación. Este número, que está en el rango [0,1], indica cuán preciso es el clasificador (precisión) y qué tan robusto es (recordar).Cuanto mayor sea el puntaje F1, mejor será el rendimiento general del modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/f1.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando el área bajo la curva ROC                                                       \n",
    "\n",
    "\n",
    "El área bajo la   curva de Características operativas receptoras ( ROC ) es una de las métricas más utilizadas para la evaluación de problemas de clasificación binaria.La mayoría de los clasificadores producen una puntuación en el rango [0,1] y no directamente como una etiqueta de clasificación. \n",
    "\n",
    "El puntaje debe ser acotado para decidir la clasificación. Un umbral natural es clasificarlo como positivo cuando el puntaje es mayor a 0.5 y negativo de lo contrario, pero esto no siempre es lo que nuestra aplicación quiere (piense en la identificación de personas con una enfermedad).Variar el umbral cambiará el rendimiento del clasificador, variando el número de TP, FP, TN y FN y, por lo tanto, el rendimiento de clasificación general.Los resultados de las variaciones de umbral se pueden tener en cuenta al trazar la curva ROC. \n",
    "\n",
    "La curva ROC tiene en cuenta la tasa de falsos positivos (especificidad) y la tasa de verdaderos positivos (sensibilidad): los problemas de clasificación binaria son una compensación entre estos dos valores. Podemos describir estos valores de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sensibilidad : la tasa positiva verdadera se define como la proporción de puntos de datos positivos que se consideran correctamente positivos, con respecto a todos los puntos de datos positivos:\n",
    "\n",
    "<img src=\"images/sensibilidad.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Especificidad : La tasa de falsos positivos se define como la proporción de puntos de datos negativos que se consideran positivos, con respecto a todos los puntos de datos negativos:\n",
    "\n",
    "<img src=\"images/especificidad.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El AUC es el área bajo la curva ROC, y se obtiene variando el umbral de clasificación:\n",
    "\n",
    "<img src=\"images/roc.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva ROC obtenida variando el umbral de clasificación. Las líneas discontinuas representan la expectativa de adivinanzas aleatorias. Está claro que tanto TPR como FPR tienen valores en el rango [0,1], y el gráfico se dibuja variando el umbral de clasificación del clasificador para obtener diferentes pares de TPR y FPR para cada valor umbral. El AUC también está en el rango [0,1] y cuanto mayor sea el valor, mejor será el modelo.Si estamos interesados ​​en medir el rendimiento de la precisión y el recuerdo de un regresor y todos los datos que se obtuvieron de la matriz de confusión son inútiles, entonces tenemos que usar otras métricas para medir el error de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todas las métricas\n",
    "\n",
    "<img src=\"images/metricas.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
