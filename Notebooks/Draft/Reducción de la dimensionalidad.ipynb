{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron\">\n",
    "  <h1><i class=\"fa fa-bar-chart\" aria-hidden=\"true\"></i> Reducción de la dimensionalidad </h1>\n",
    "  <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducción\n",
    "Muchos de los problemas de Machine Learning implican miles o incluso millones de funciones para cada instancia de capacitación. No solo todas estas características hacen que el entrenamiento sea extremadamente lento, sino que también pueden hacer que sea mucho más difícil encontrar una buena solución, como veremos. Este problema a menudo se conoce como la __maldición de la dimensionalidad__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Advertencia</b>\n",
    "\n",
    "\n",
    "La reducción de la dimensionalidad causa cierta pérdida de información (al igual que comprimir una imagen en JPEG puede degradar su calidad), por lo que, aunque acelerará el entrenamiento, puede hacer que su sistema funcione un poco peor. También hace que sus tuberías sean un poco más complejas y, por lo tanto, más difíciles de mantener. Por lo tanto, si el entrenamiento es demasiado lento, primero debe intentar entrenar su sistema con los datos originales antes de considerar el uso de la reducción de dimensionalidad. En algunos casos, la reducción de la dimensionalidad de los datos de entrenamiento puede filtrar algunos ruidos y detalles innecesarios y, por lo tanto, generar un mayor rendimiento, pero en general no lo hará; solo acelerará el entrenamiento.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde un punto de vista estadístico, vivimos en un mundo multidimensional, con una gran cantidad de características explicativas que modelan a cada objeto y evento en el universo. Problemas que involucran el concepto de multidimensionalidad son muy comunes en áreas como el Análisis de Datos Multivariantes y Bioinformática. En estadística, es común escuchar sobre el problema que se presenta cuando la cantidad de datos a estudiar no es significativa. Este problema se complica si el número de variables de cada muestra es muy grande (pocos datos y muchos atributos). La peor situación surge cuando la\n",
    "cantidad de características que describen a cada instancia supera al número de elementos en el conjunto de datos. \n",
    "\n",
    "\n",
    "Estadísticamente es necesario que el número de elementos en el conjunto de datos crezca de forma exponencial con respecto al número de variables. En la práctica,por lo general esta situación no se cumple [Cunningham, 2007]. Otro factor a tomar en\n",
    "cuenta es cuando el n´umero de caracter´ısticas es muy grande, entonces la probabilidad\n",
    "de que exista redundancia entre ellas es alta. Podemos entender como redundancia a la\n",
    "correlación entre estas características. \n",
    "\n",
    "Por lo general, se prefiere un conjunto de datos sin\n",
    "correlación entre sus variables, esto hace que exista menor o, en el caso óptimo, redundancia cero. Dicha eliminación de características redundantes se aplica a grandes conjuntos\n",
    "de objetos y se lleva a cabo mediante un análisis de los datos. En este análisis se extrae la\n",
    "información más relevante que se encuentra oculta entre sus variables descriptivas. Como\n",
    "consecuencia, en este proceso se detecta que variables son relevantes y la forma en que\n",
    "interactúan unas con otras. Al detectar que variables son relevantes se evita que en el\n",
    "proceso se pierda información importante. \n",
    "\n",
    "\n",
    "La extracción y selección de información en\n",
    "un conjunto de datos consiste en reformular el conjunto usando menos características, las\n",
    "cuales deben de ser capaces de conservar las propiedades del conjunto de datos original.\n",
    "A este proceso se le conoce como reducción de dimensionalidad [Venna et al., 2010].\n",
    "Cuando se aplica un método de reducción de dimensionalidad a un conjunto de datos,\n",
    "la información descubierta y extraída también sirve de ayuda para interpretar y clasificar\n",
    "los datos existentes, de tal forma que dicha interpretación y clasificación sea similar o\n",
    "mejor que la correspondiente utilizando el conjunto original [Cunningham, 2007, Lee and\n",
    "Verleysen, 2007]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maldición de la dimensionalidad\n",
    "\n",
    "La maldición de la dimensionalidad [Bellman and Bellman, 1961] se refiere a una serie de dificultades que ocurren cuando se trata con datos de alta dimensión. Normalmente, cuando se tiene un conjunto de datos pequeños (pocas observaciones) con muchas características que lo describen, el espacio de alta dimensionalidad donde residen éstos es\n",
    "disperso. Lo anterior origina lo que se conoce como __fenómeno del espacio vacío__ [Scott and Thompsonm, 1983], el cual se asocia con el hecho de que el número de muestras requeridas para estimar una función de varias variables con una exactitud dada sobre un dominio específico debe crecer exponencialmente de acuerdo al número de dimensiones.\n",
    "\n",
    "\n",
    "Esto último es ampliamente conocido como maldición de la dimensionalidad, la cual aunado a espacios dispersos (fenómeno del espacio vacío) conduce a propiedades inesperadas del análisis de datos que residen en espacios de alta dimensión.\n",
    "Cuando la dimensión crece, las propiedades de los espacios _Euclidianos_ ya no son validas, la longitud de los vectores tienden a normalizarse entre m´as se incremente el número de dimensiones en el que se plantea, este fenómeno se le conoce como el __fenómeno de la concentración__ , el cual nos indica la falta de una métrica adecuada en espacios multidimensionales grandes. Dicho fenómeno hace que por ejemplo, el problema de clasificación basado en la búsqueda del vecino más cercano (Nearest-Neighbor Search) sea un problema difícil de resolver en espacios altamente dimensionales, debido a que la distancia _Euclidiana_ entre cualesquiera dos vectores es aproximadamente constante [Bellman and Bellman,\n",
    "1961]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosotros estamos tan acostumbrados a vivir en tres dimensiones por lo que nuestra intuición nos falla cuando intentamos imaginar un espacio de alta dimensión. Incluso un hipercubo 4D básico es increíblemente difícil de imaginar en nuestras mentes (ver Figura ), y mucho menos un elipsoide 200-dimensional doblado en un espacio de 1,000 dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rd_1.png\" alt=\"Drawing\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencia y relevancia de variables\n",
    "\n",
    "Una solución para atenuar o contrarrestar el problema antes mencionado en la anterior se logra encontrando las variables (características) que contienen dependencia entre ellas. Cuando dos o más variables están altamente correlacionadas, una de ellas contiene información sobre las otras, así, se puede conservar las variables que contienen la mayor información relevante y suprimir el resto de variables irrelevantes de las características del objeto. Sin embargo, hacer esto no garantiza conservar la información más relevante que se transmiten entre ellas. Por lo general, la correlación entre variables es muy compleja.\n",
    "\n",
    "Otra forma de reducir el número de variables es encontrar un conjunto transformado de estas que logre representar la información más relevante del conjunto original, con la menor perdida de información. Aquí, se busca que el conjunto transformado contenga un número menor de variables.\n",
    "\n",
    "Al realizar una transformación se busca nuevas variables con propiedades bien definidas, las cuales aseguran que la transformación no altere la información contenida y transferida del conjunto original que ahora está representada de diferente forma. De acuerdo con el modelo de datos que se esté manejando, se debe de seleccionar una transformación o proyección adecuada. La proyección tiene como primer objetivo eliminar dependencia.\n",
    "\n",
    "Esto es, la reducción se lleva acabo con el fin de reducir el número de variables y trata de eliminar la redundancia en el conjunto original. El segundo y más complejo objetivo de una proyección, es recuperar la denominada variable latente, la que se esconde dentro de las variables originales y la cual no es posible medir directamente. Esta tarea recibe el nombre de separación de la variable latente, también es conocida como separación ciega de fuentes en el área de procesamiento de señales, o análisis de componentes independientes en análisis de datos multivariantes [Bellman and Bellman, 1961]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicaciones de la reducción de dimensionalidad\n",
    "\n",
    "El principal inconveniente cuando nos enfrentamos a datos con alta dimensionalidad es que esto afecta el rendimiento de los algoritmos de aprendizaje automático y reconocimiento de patrones. El número de variables presentes en una instancia determina el tamaño del espacio de hipótesis, esto es, conforme aumenta el número de características que describen a los objetos, el tamaño del espacio solución crece exponencialmente. Para solucionar este problema se aplican técnicas de RD, las cuales pueden lograr mejorar el rendimiento de un algoritmo, facilitar la interpretación y análisis de los resultados y reducir el tiempo de ejecución de dichos algoritmos [Cunningham, 2007].\n",
    "\n",
    "La reducción de la dimensión se puede asociar a tres tareas:\n",
    "\n",
    "- La visualización y exploración.\n",
    "- La regresión.\n",
    "- La clasificación.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La visualización y exploración parte de un conjunto de datos residente en un espacio de alta dimensión y tiene como finalidad proyectarlo a un espacio de pocas dimensiones (usualmente dos o tres) mediante la preservación de sus propiedades intrínsecas. El conjunto resultante de baja dimensión debe poder representarse de forma gráfica para su exploración visual. En contraste, en una regresión, la reducción de dimensionalidad debe ayudar a que la regresión se comporte con un error mínimo cuando es necesario predecir nuevos puntos. De forma análoga, el objetivo de la clasificación es, después de aplicar la reducción a un conjunto, producir el mínimo error de clasificación. En [Lee and Verleysen,2007] se afirma que los métodos de reducción de dimensionalidad pueden ser capaces de:\n",
    "    \n",
    "- Estimar el número de variables latentes.\n",
    "- Reducir la dimensionalidad mediante una proyección de los datos.\n",
    "- Recuperar variables latentes mediante una proyección de los datos.\n",
    "\n",
    "Las variables latentes son las variables que no se observan directamente sino que son inferidas a partir de otras variables. Para encontrar el número de variables latentes, es necesario realizar una estimación de la dimensión intrínseca. Sin embargo, no todos los métodos son capaces de poder realizar esta estimación. Durante la reducción de dimensionalidad interesa capturar las variables latentes, y descartar las variables que involucran ruido y otras imperfecciones. El segundo paso es proyectar los datos de alta dimensión en una dimensión inferior, con el objetivo de lograr una representación compacta y facilitar el posterior tratamiento de los datos. Dicho de otra forma, lo que se busca es la visualización y/o compresión de los datos. Por último, la tarea de separación de variables latentes también implica medios para recuperar las variables, con el fin de cumplir con un objetivo más allá de sólo una reducción de la dimensionalidad.\n",
    "\n",
    "En este proceso se imponen restricciones adicionales al nuevo espacio de baja dimensión. Por ejemplo, es común que los algoritmos de RD al realizar la tarea de recuperación de variables latentes modelen las nuevas variables del espacio de baja dimensión como una combinación lineal de las latentes para garantizar que los datos cumplan con cierto criterio, como puede ser independencia estadística.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de reducción de dimensionalidad\n",
    "Existen dos principales enfoques para la reducción de la dimensionalidad, aunque ambos reducen el conjunto de características, el primero de ellos lo hace transformando el conjunto de variables originales y el segundo enfoque selecciona un subconjunto de estas variables sin alterarlas [Cunningham, 2007]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RD mediante transformación de características\n",
    "\n",
    "Al utilizar este enfoque se transforman las características originales de tal forma que se encuentra un nuevo grupo de objetos con el mismo número de instancias, pero con menor número de características descriptivas. En dicha transformación se busca que el subconjunto de características (que es diferente al original), contenga la información más relevante que almacenan las variables iniciales. En consecuencia, el nuevo conjunto es una representaci´on del conjunto original. Esta técnica puede dividirse en dos subcategorias [Cunningham, 2007].\n",
    "\n",
    "__Extracción de Características (EC)__ : Consiste en producir un nuevo conjunto de características aplicando un mapeo a los datos originales. El análisis de componentes principales y análisis discriminante lineal son los dos algoritmos más conocidos que realizan extracci´on de características mediante aprendizaje no supervisado y supervisado, respectivamente.\n",
    "\n",
    "__Generación de Características (GC)__ : Primero se encuentra la información oculta (o perdida) entre las características del conjunto y luego se incrementa el tamaño del espacio de hipótesis generando nuevas características, las cuales sólo almacenan datos que enfatizan la nueva información descubierta.\n",
    "\n",
    "La EC es la técnica más utilizada, debido a su capacidad para reducir el conjunto de características. Es lógico pensar que la generación de nuevas características no es la mejor opción para reducir dimensionalidad, pues obliga al conjunto de variables a expandirse. Sin embargo, después de aplicar el método de GC podemos aplicar un método de extracción\n",
    "(EC) para obtener un subconjunto de características útil.\n",
    "\n",
    "### RD mediante selección de características\n",
    "\n",
    "El objetivo de este enfoque es encontrar el mejor subconjunto (mínimo) de características, el cual debe ofrecer resultados similares o mejores dependiendo del problema al que se enfrente. La ventaja de este enfoque es que las características seleccionadas representan el concepto físico o abstracto original de su variable correspondiente, y pueden ser interpretadas directamente. Por el contrario, en las técnicas de transformación las nuevas variables, por lo general, no pueden ser interpretadas directamente ya que representa conceptos y métricas desconocidas. Usualmente, un modelo predictivo se utiliza para evaluar todas las posibles combinaciones de características y asignar una puntuación basada en la exactitud del modelo (aprendizaje supervisado). Los sistemas de evaluación pueden ser de carácter supervisado y no supervisado, en ambos casos se dividen en tres categorías.\n",
    "\n",
    "\n",
    "Para el aprendizaje supervisado estas categorías consisten en lo siguiente.\n",
    "\n",
    "__Enfoque de empaquetado (wrapper)__ : El criterio de selección está basado en la exactitud dada por un clasificador.\n",
    "\n",
    "__Enfoque de filtro (filter)__ : El criterio de selección está basado en una función distinta a la precisión de un clasificador, por ejemplo, una función ranking o criterio de separabilidad de clases.\n",
    "\n",
    "__Enfoque embebido (embedded)__ : Utiliza un medida de perdida o ganancia de información para elegir las mejores características. Dicha medida se encuentra incorporada en el método de aprendizaje utilizado.\n",
    "\n",
    "Existen métodos de selección de características no supervisados, sin embargo es una zona menos explorada. La razón es que su objetivo es menos claro y es complicado encontrar un número reducido de características cuando el número de grupos (cluster) a crear es desconocido [Bellman and Bellman, 1961]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de reducción de dimensionalidad\n",
    "\n",
    "[Sarlin and Peltonen, 2011] hacen una división de los métodos de reducción en dos generaciones. \n",
    "\n",
    "La primera generación se compone de los métodos clásicos que aún son ampliamente aceptados y difundidos en distintas áreas de las ciencias. Estos métodos tienen como objetivo, dado un conjunto de datos que reside en un espacio de alta dimensión, proyectar cada uno de los puntos (datos) a un espacio de menor dimensión basándose en la preservación de las distancias. \n",
    "\n",
    "La segunda generación es un grupo de métodos menos homogéneo que van desde las llamadas técnicas espectrales hasta las técnicas basadas en grafos. Debido a la gran cantidad de métodos, sólo se mencionan algunos de los más utilizados pertenecientes a ambas generaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proyección\n",
    "\n",
    "\n",
    "En la mayoría de los problemas del mundo real, las instancias de capacitación no se distribuyen de manera uniforme en todas las dimensiones. Muchas características son casi constantes, mientras que otras están altamente correlacionadas. Como resultado, todas las instancias de entrenamiento se encuentran dentro (o cerca de) un subespacio mucho más bajo del espacio de alta dimensión.\n",
    "\n",
    "<img src=\"images/proy_1.png\" height=\"300\" width=\"600\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que todas las instancias de entrenamiento se encuentran cerca de un plano: este es un subespacio de menor dimensión (2D) del espacio de alta dimensión (3D). Si proyectamos cada instancia de entrenamiento perpendicularmente en este subespacio (como se representa por las líneas cortas que conectan las instancias al plano), obtenemos el nuevo conjunto de datos 2D que se muestra en la siguiente figura. Acabamos de reducir la dimensionalidad del conjunto de datos de 3D a 2D. Tenga en cuenta que los ejes corresponden a las nuevas características $z_1$ y $z_2$ (las coordenadas de las proyecciones en el plano)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/proy_2.png\" height=\"300\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manifold learning\n",
    "\n",
    "\n",
    "Sin embargo, la proyección no siempre es el mejor enfoque para la reducción de la dimensionalidad. En muchos casos, el subespacio puede girar y girar, como en el famoso conjunto de datos Swiss roll representado a continuación.\n",
    "\n",
    "<img src=\"images/proy_3.png\" height=\"300\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplemente proyectar en un plano (por ejemplo, soltando x 3 ) aplastaría diferentes capas de Swiss roll, como se muestra en el lado izquierdo. Lo que realmente desea es desenrollar los datos para obtener el conjunto de datos 2D como en el lado derecho de la Figura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/proy_4.png\" height=\"300\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swiss roll es un ejemplo de una variedad 2D . En pocas palabras, un múltiple 2D es una forma 2D que se puede doblar y torcer en un espacio de dimensiones superiores. En términos más generales, una variedad d -dimensional es una parte de un espacio n -dimensional (donde $d < n$ ) que localmente se parece a un hiperplano d -dimensional. En el caso del Swiss roll, $d = 2$ y $n = 3$ : localmente se asemeja a un plano 2D, pero se rueda en la tercera dimensión.\n",
    "\n",
    "\n",
    "Muchos algoritmos de reducción de dimensionalidad funcionan modelando la variedad en la que se encuentran las instancias de entrenamiento; Esto se llama Manifold learning . Se basa en el supuesto múltiple , también llamado la hipótesis múltiple , que sostiene que la mayoría de los conjuntos de datos de alta dimensión del mundo real se encuentran cerca de una variedad de dimensiones mucho más bajas. Esta suposición se observa muy a menudo empíricamente.\n",
    "\n",
    "\n",
    "El supuesto múltiple a menudo va acompañado de otro supuesto implícito: que la tarea en cuestión (por ejemplo, clasificación o regresión) será más simple si se expresa en el espacio de dimensiones inferiores del múltiple. Por ejemplo, en la fila superior de la Figura , el Swiss roll se divide en dos clases: en el espacio 3D (a la izquierda), el límite de decisión sería bastante complejo, pero en el espacio múltiple desenrollado 2D (a la derecha ), el límite de decisión es una línea recta. Sin embargo, esta suposición implícita no siempre es válida. Por ejemplo, en la fila inferior de la Figura , el límite de decisión se encuentra en $x_1 = 5$ . \n",
    "\n",
    "\n",
    "Este límite de decisión parece muy simple en el espacio 3D original (un plano vertical), pero se ve más complejo en el múltiple desenrollado (una colección de cuatro segmentos de línea independientes).En resumen, reducir la dimensionalidad de su conjunto de entrenamiento antes de entrenar un modelo generalmente acelerará el entrenamiento, pero no siempre puede conducir a una solución mejor o más simple; Todo depende del conjunto de datos.Esperemos que ahora tenga una buena idea de cuál es la maldición de la dimensionalidad y cómo los algoritmos de reducción de dimensionalidad pueden combatirla, especialmente cuando se cumple la suposición múltiple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/proy_5.png\" height=\"300\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas técnicas de reducción de dimensionalidad\n",
    "\n",
    "- PCA\n",
    "- LDA\n",
    "- Factor Analysis\n",
    "- ICA\n",
    "- t-SNE\n",
    "- Random Forest\n",
    "- ISOMAP\n",
    "- BPDR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
