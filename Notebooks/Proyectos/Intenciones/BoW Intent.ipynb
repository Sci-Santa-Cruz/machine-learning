{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intenciones__ : el sistema de Inteligencia Artificial, lo primero que trata de identificar cuando alguien le escribe es la “intención” de lo que el usuario le ha querido decir. Por ejemplo, si estamos delante de un chatbot para reservar entradas, habremos definido diferentes intenciones propias del negocio como _#ComprarEntrada_ , _#ModificarReserva_ , _#DevoluciónEntrada_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SantaCruz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SantaCruz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la primera vez que se imprtar NTKL se debe ejecuatr la siguiente setencia para descargar recursos que utiliza dicha biblioteca\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para estanadarizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza(word):\n",
    "        # La palabra debe ser en minúsculas\n",
    "        # elimina tildes \n",
    "        # elimina mayúsculas y números\n",
    "        from unicodedata import normalize, category\n",
    "        return ''.join([x for x in normalize('NFD', word) if category(x) == 'Ll'])\n",
    "    \n",
    "def normalizar_string(cadena):\n",
    "        #normalizar un cadena (secuencia de palabras)\n",
    "        result = \"\"\n",
    "        # convertir a minúsucla y eliminar espacios en blanco extras\n",
    "        cadena = cadena.lower().strip()\n",
    "\n",
    "        for word in cadena.split(\" \"):\n",
    "            aux = normaliza(word)\n",
    "            if len(aux) > 0:\n",
    "                result = result + ' ' + aux.strip()\n",
    "            \n",
    "        return result.strip(' ').strip('\\n')\n",
    "    \n",
    "def foo(text):\n",
    "         #eliminar signos de puntuación\n",
    "        forbidden = {\"?\", \"¿\", \"¡\", \"!\", \" \", \",\", \".\", \";\", \":\", \"-\", \"_\"}\n",
    "        return \"\".join(c for c in text.lower() if c not in forbidden )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer el data set para realizar el pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentencia</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sería tan amable de poderme comunicar con la p...</td>\n",
       "      <td>__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Estoy buscando por favor a de buró de crédito</td>\n",
       "      <td>buro-credito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>con de contabilidad</td>\n",
       "      <td>contabilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sería tan amable de comunicarme con la contabi...</td>\n",
       "      <td>contabilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>habla de DUMMY sería tan amable de comunicarme...</td>\n",
       "      <td>contabilidad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentencia         class\n",
       "0  sería tan amable de poderme comunicar con la p...            __\n",
       "1      Estoy buscando por favor a de buró de crédito  buro-credito\n",
       "2                                con de contabilidad  contabilidad\n",
       "3  sería tan amable de comunicarme con la contabi...  contabilidad\n",
       "4  habla de DUMMY sería tan amable de comunicarme...  contabilidad"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visulizar los primeros elementos\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraer el contenido del data set como una lista de listas,  sin formato Dataframe\n",
    "X = df.sentencia.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sería tan amable de poderme comunicar con la persona encargada de solicitar un correo electrónico'\n",
      " 'Estoy buscando por favor a de buró de crédito' 'con de contabilidad'\n",
      " 'sería tan amable de comunicarme con la contabilidad por favor '\n",
      " 'habla de DUMMY sería tan amable de comunicarme a contabilidad']\n"
     ]
    }
   ],
   "source": [
    "#imprmir los primeros 5 elementos de la lista\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar la representación de la columna de etiquetas, de  su forma  categórica-texto a categórica-numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir clases categoricas 'string' a discretas 'entero'\n",
    "clases = pd.DataFrame(df['class'].astype('category'))\n",
    "clases = clases.apply(lambda x: x.cat.codes)\n",
    "Y = clases.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]], dtype=int8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seria tan amable de poderme comunicar con la persona encargada de solicitar un correo electronico\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "sentencia = normalizar_string(X[0])\n",
    "print(sentencia)\n",
    "print(len(sentencia.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sentencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seria',\n",
       " 'tan',\n",
       " 'amable',\n",
       " 'de',\n",
       " 'poderme',\n",
       " 'comunicar',\n",
       " 'con',\n",
       " 'la',\n",
       " 'persona',\n",
       " 'encargada',\n",
       " 'de',\n",
       " 'solicitar',\n",
       " 'un',\n",
       " 'correo',\n",
       " 'electronico']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Eliminar stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words en español: \n",
      " ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con']\n"
     ]
    }
   ],
   "source": [
    "#load stop words\n",
    "from nltk.corpus import stopwords\n",
    "spanish_stopwords = stopwords.words('spanish') #stop words precargas  de la biblioteca nltk\n",
    "print(\"Stop words en español: \\n\",spanish_stopwords[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "con\n",
      "la\n",
      "de\n",
      "un\n"
     ]
    }
   ],
   "source": [
    "words_ = []\n",
    "\n",
    "for word in words: # iterate over word_list\n",
    "    if word in spanish_stopwords:\n",
    "        print(word)\n",
    "        #while word in words : words.remove(word) #puede existir más de una incidencia\n",
    "    else :\n",
    "        words_.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words_\n",
    "del words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentencia :  10 \n",
      "  ['tan', 'amable', 'poderme', 'electronico', 'comunicar', 'encargada', 'persona', 'solicitar', 'seria', 'correo']\n"
     ]
    }
   ],
   "source": [
    "words = list(set(words)) #uniques\n",
    "print (\" Sentencia : \",len(words), \"\\n \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'tan' in spanish_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tan'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_stopwords.append('tan')\n",
    "spanish_stopwords[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemizar texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solicit', 'person', 'pod', 'tan', 'comunic', 'electron', 'corre', 'seri', 'amabl', 'encarg']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#stemmer = LancasterStemmer()\n",
    "stemmer = SnowballStemmer('spanish', ignore_stopwords=True)\n",
    "\n",
    "ignore_words = ['?']\n",
    "\n",
    "#stemizar, pasar a minúscula y eliminar duplicadas \n",
    "words = [stemmer.stem(w) for w in words if w not in ignore_words]\n",
    "words = list(set(words))\n",
    "\n",
    "\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación a nivel de palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from  numpy  import  array \n",
    "#from  numpy  import  argmax \n",
    "#from  sklearn . preprocessing  import  LabelEncoder \n",
    "#from  sklearn . preprocessing  import  OneHotEncoder\n",
    "\n",
    "#values =array(words) \n",
    "#print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode \n",
    "#label_encoder = LabelEncoder( ) \n",
    "#integer_encoded = label_encoder . fit_transform ( values ) \n",
    "#print ( integer_encoded ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary encode \n",
    "#onehot_encoder   =   OneHotEncoder ( sparse = False ) \n",
    "#integer_encoded   =   integer_encoded . reshape ( len ( integer_encoded ) ,   1 ) \n",
    "#onehot_encoded   =   onehot_encoder . fit_transform ( integer_encoded.reshape(-1, 1) ) \n",
    "#print ( onehot_encoded ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert first example \n",
    "#inverted   =   label_encoder . inverse_transform ( [ argmax ( onehot_encoded [ 0 ,   : ] ) ] ) \n",
    "#print ( inverted ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación a nivel de sentencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_modelo_lenguaje = dict({k: v for v, k in enumerate(words)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solicit': 0,\n",
       " 'person': 1,\n",
       " 'pod': 2,\n",
       " 'tan': 3,\n",
       " 'comunic': 4,\n",
       " 'electron': 5,\n",
       " 'corre': 6,\n",
       " 'seri': 7,\n",
       " 'amabl': 8,\n",
       " 'encarg': 9}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_modelo_lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_sentence = [0] * len(words)\n",
    "for word in words :\n",
    "    encoding_sentence[dict_modelo_lenguaje[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_ = copy.deepcopy(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sería tan amable de poderme comunicar con la persona encargada de solicitar un correo electrónico',\n",
       "       'Estoy buscando por favor a de buró de crédito',\n",
       "       'con de contabilidad',\n",
       "       'sería tan amable de comunicarme con la contabilidad por favor ',\n",
       "       'habla de DUMMY sería tan amable de comunicarme a contabilidad'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_counts:  OrderedDict([('sería', 6), ('tan', 8), ('amable', 7), ('de', 77), ('poderme', 1), ('comunicar', 70), ('con', 130), ('la', 35), ('persona', 11), ('encargada', 2), ('solicitar', 7), ('un', 9), ('correo', 2), ('electrónico', 1), ('estoy', 8), ('buscando', 5), ('por', 55), ('favor', 30), ('a', 26), ('buró', 1), ('crédito', 1), ('contabilidad', 3), ('comunicarme', 23), ('habla', 3), ('dummy', 7), ('me', 96), ('podría', 21), ('el', 38), ('contador', 1), ('general', 2), ('gustaría', 2), ('mucho', 1), ('poder', 1), ('contactar', 1), ('que', 14), ('se', 6), ('encarga', 1), ('los', 3), ('arrendamientos', 1), ('puros', 1), ('y', 3), ('financieros', 1), ('es', 5), ('para', 14), ('información', 3), ('cursos', 1), ('qué', 5), ('tal', 3), ('comunicas', 4), ('dirección', 1), ('una', 23), ('licenciamiento', 1), ('informático', 1), ('discúlpame', 1), ('molestia', 3), ('da', 1), ('pena', 2), ('pero', 1), ('puedes', 22), ('disculpa', 9), ('podrías', 11), ('extensión', 9), ('busco', 6), ('soportes', 1), ('contadora', 1), ('disculpame', 2), ('prodrias', 1), ('encuentra', 4), ('empresa', 2), ('del', 4), ('corporativo', 1), ('está', 1), ('en', 3), ('esta', 1), ('oficina', 2), ('disculpe', 21), ('podras', 1), ('conectar', 1), ('podria', 5), ('señor', 12), ('puede', 16), ('transferir', 1), ('señorita', 3), ('si', 8), ('seria', 2), ('éste', 2), ('ahí', 1), ('intentando', 4), ('hablo', 2), ('comunica', 4), ('sistemas', 7), ('comunico', 1), ('iban', 1), ('podes', 1), ('cominicar', 1), ('precio', 1), ('sí', 2), ('gracias', 2), ('apoyándome', 1), ('comunicándome', 2), ('oye', 2), ('licenciada', 1), ('ayudar', 2), ('puedo', 2), ('buscó', 1), ('nuevamente', 1), ('molestándote', 1), ('perdón', 1), ('quisiera', 3), ('hablar', 10), ('tendrán', 3), ('su', 2), ('número', 4), ('celular', 2), ('podré', 2), ('pude', 1), ('onda', 1), ('ingeniero', 2), ('quería', 3), ('quién', 4), ('marco', 1), ('dejando', 1), ('este', 1), ('numero', 1), ('como', 1), ('referencia', 12), ('quiero', 8), ('necesito', 3), ('comunique', 1), ('oiga', 1), ('yo', 2), ('no', 2), ('sé', 1), ('poner', 1), ('llamada', 1), ('te', 1), ('trató', 2), ('hablé', 1), ('quiciera', 1), ('dejar', 3), ('mensaje', 2), ('voz', 3), ('mesaje', 1), ('servicio', 3), ('buzón', 1), ('alguien', 6), ('mercadotecnia', 1), ('ofimática', 3), ('proporcionar', 1), ('área', 11), ('informática', 1), ('podrás', 1), ('canalizar', 1), ('revisa', 1), ('proveedores', 1), ('mi', 1), ('nombre', 1), ('ofrecer', 1), ('mis', 1), ('productos', 1), ('servicios', 2), ('mire', 2), ('le', 1), ('llamó', 1), ('estamos', 1), ('ofreciendo', 1), ('las', 4), ('personas', 2), ('referencias', 10), ('comunicame', 3), ('recursos', 10), ('humanos', 10), ('donde', 2), ('dan', 2), ('laboral', 9), ('laborales', 4), ('laboró', 3), ('ustedes', 6), ('trabajó', 3), ('habló', 2), ('ha', 2), ('comentado', 2), ('sobre', 3), ('apoyar', 1), ('apoye', 1), ('ver', 2), ('lo', 2), ('deuna', 1), ('vacante', 3), ('sus', 1), ('vacantes', 1), ('bh', 1), ('dame', 2), ('segundo', 2), ('rh', 1), ('jovencita', 1), ('hacer', 1), ('al', 4), ('xannan', 2), ('encargado', 2), ('debo', 1), ('voy', 1), ('soporte', 6), ('técnico', 1), ('jannan', 1), ('area', 1), ('telecomunicaciones', 1), ('estado', 1), ('tratando', 2), ('localizar', 2), ('gerente', 2), ('tesorería', 2), ('ventas', 4), ('licencias', 1), ('contacto', 1), ('compras', 1), ('conde', 1), ('departamento', 2), ('respecto', 2), ('compra', 2), ('software', 2), ('ayudarme', 1), ('asesor', 1), ('comercial', 1), ('usted', 1), ('gentil', 1), ('vigilancia', 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(X)\n",
    "\n",
    "# summarize what was learned\n",
    "print(\"word_counts: \", t.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_count:  192\n"
     ]
    }
   ],
   "source": [
    "print(\"document_count: \",t.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index:  {'con': 1, 'me': 2, 'de': 3, 'comunicar': 4, 'por': 5, 'el': 6, 'la': 7, 'favor': 8, 'a': 9, 'comunicarme': 10, 'una': 11, 'puedes': 12, 'podría': 13, 'disculpe': 14, 'puede': 15, 'que': 16, 'para': 17, 'señor': 18, 'referencia': 19, 'persona': 20, 'podrías': 21, 'área': 22, 'hablar': 23, 'referencias': 24, 'recursos': 25, 'humanos': 26, 'un': 27, 'disculpa': 28, 'extensión': 29, 'laboral': 30, 'tan': 31, 'estoy': 32, 'si': 33, 'quiero': 34, 'amable': 35, 'solicitar': 36, 'dummy': 37, 'sistemas': 38, 'sería': 39, 'se': 40, 'busco': 41, 'alguien': 42, 'ustedes': 43, 'soporte': 44, 'buscando': 45, 'es': 46, 'qué': 47, 'podria': 48, 'comunicas': 49, 'encuentra': 50, 'del': 51, 'intentando': 52, 'comunica': 53, 'número': 54, 'quién': 55, 'las': 56, 'laborales': 57, 'al': 58, 'ventas': 59, 'contabilidad': 60, 'habla': 61, 'los': 62, 'y': 63, 'información': 64, 'tal': 65, 'molestia': 66, 'en': 67, 'señorita': 68, 'quisiera': 69, 'tendrán': 70, 'quería': 71, 'necesito': 72, 'dejar': 73, 'voz': 74, 'servicio': 75, 'ofimática': 76, 'comunicame': 77, 'laboró': 78, 'trabajó': 79, 'sobre': 80, 'vacante': 81, 'encargada': 82, 'correo': 83, 'general': 84, 'gustaría': 85, 'pena': 86, 'disculpame': 87, 'empresa': 88, 'oficina': 89, 'seria': 90, 'éste': 91, 'hablo': 92, 'sí': 93, 'gracias': 94, 'comunicándome': 95, 'oye': 96, 'ayudar': 97, 'puedo': 98, 'su': 99, 'celular': 100, 'podré': 101, 'ingeniero': 102, 'yo': 103, 'no': 104, 'trató': 105, 'mensaje': 106, 'servicios': 107, 'mire': 108, 'personas': 109, 'donde': 110, 'dan': 111, 'habló': 112, 'ha': 113, 'comentado': 114, 'ver': 115, 'lo': 116, 'dame': 117, 'segundo': 118, 'xannan': 119, 'encargado': 120, 'tratando': 121, 'localizar': 122, 'gerente': 123, 'tesorería': 124, 'departamento': 125, 'respecto': 126, 'compra': 127, 'software': 128, 'poderme': 129, 'electrónico': 130, 'buró': 131, 'crédito': 132, 'contador': 133, 'mucho': 134, 'poder': 135, 'contactar': 136, 'encarga': 137, 'arrendamientos': 138, 'puros': 139, 'financieros': 140, 'cursos': 141, 'dirección': 142, 'licenciamiento': 143, 'informático': 144, 'discúlpame': 145, 'da': 146, 'pero': 147, 'soportes': 148, 'contadora': 149, 'prodrias': 150, 'corporativo': 151, 'está': 152, 'esta': 153, 'podras': 154, 'conectar': 155, 'transferir': 156, 'ahí': 157, 'comunico': 158, 'iban': 159, 'podes': 160, 'cominicar': 161, 'precio': 162, 'apoyándome': 163, 'licenciada': 164, 'buscó': 165, 'nuevamente': 166, 'molestándote': 167, 'perdón': 168, 'pude': 169, 'onda': 170, 'marco': 171, 'dejando': 172, 'este': 173, 'numero': 174, 'como': 175, 'comunique': 176, 'oiga': 177, 'sé': 178, 'poner': 179, 'llamada': 180, 'te': 181, 'hablé': 182, 'quiciera': 183, 'mesaje': 184, 'buzón': 185, 'mercadotecnia': 186, 'proporcionar': 187, 'informática': 188, 'podrás': 189, 'canalizar': 190, 'revisa': 191, 'proveedores': 192, 'mi': 193, 'nombre': 194, 'ofrecer': 195, 'mis': 196, 'productos': 197, 'le': 198, 'llamó': 199, 'estamos': 200, 'ofreciendo': 201, 'apoyar': 202, 'apoye': 203, 'deuna': 204, 'sus': 205, 'vacantes': 206, 'bh': 207, 'rh': 208, 'jovencita': 209, 'hacer': 210, 'debo': 211, 'voy': 212, 'técnico': 213, 'jannan': 214, 'area': 215, 'telecomunicaciones': 216, 'estado': 217, 'licencias': 218, 'contacto': 219, 'compras': 220, 'conde': 221, 'ayudarme': 222, 'asesor': 223, 'comercial': 224, 'usted': 225, 'gentil': 226, 'vigilancia': 227}\n"
     ]
    }
   ],
   "source": [
    "print(\"word_index: \", t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_docs:  {'poderme': 1, 'la': 33, 'electrónico': 1, 'persona': 11, 'con': 128, 'encargada': 2, 'sería': 6, 'comunicar': 70, 'de': 67, 'solicitar': 7, 'correo': 2, 'un': 9, 'amable': 7, 'tan': 8, 'buscando': 5, 'favor': 30, 'a': 25, 'crédito': 1, 'por': 54, 'estoy': 8, 'buró': 1, 'contabilidad': 3, 'comunicarme': 23, 'dummy': 7, 'habla': 3, 'me': 93, 'contador': 1, 'podría': 21, 'general': 2, 'el': 38, 'gustaría': 2, 'mucho': 1, 'se': 6, 'financieros': 1, 'que': 12, 'puros': 1, 'y': 3, 'encarga': 1, 'poder': 1, 'arrendamientos': 1, 'los': 3, 'contactar': 1, 'cursos': 1, 'para': 14, 'información': 3, 'es': 5, 'dirección': 1, 'qué': 5, 'comunicas': 4, 'tal': 3, 'una': 23, 'licenciamiento': 1, 'informático': 1, 'discúlpame': 1, 'da': 1, 'pero': 1, 'molestia': 3, 'puedes': 22, 'pena': 2, 'disculpa': 9, 'podrías': 11, 'extensión': 9, 'busco': 6, 'soportes': 1, 'contadora': 1, 'prodrias': 1, 'disculpame': 2, 'encuentra': 4, 'empresa': 2, 'del': 4, 'corporativo': 1, 'en': 3, 'esta': 1, 'está': 1, 'oficina': 2, 'disculpe': 21, 'podras': 1, 'conectar': 1, 'señor': 12, 'podria': 5, 'puede': 16, 'transferir': 1, 'señorita': 3, 'si': 8, 'seria': 2, 'éste': 2, 'ahí': 1, 'intentando': 4, 'hablo': 2, 'comunica': 4, 'sistemas': 7, 'comunico': 1, 'iban': 1, 'podes': 1, 'cominicar': 1, 'precio': 1, 'sí': 2, 'gracias': 2, 'apoyándome': 1, 'comunicándome': 2, 'oye': 2, 'licenciada': 1, 'ayudar': 2, 'puedo': 2, 'buscó': 1, 'nuevamente': 1, 'molestándote': 1, 'quisiera': 3, 'hablar': 10, 'perdón': 1, 'tendrán': 3, 'número': 4, 'su': 2, 'celular': 2, 'podré': 2, 'pude': 1, 'onda': 1, 'ingeniero': 2, 'quería': 3, 'este': 1, 'referencia': 12, 'dejando': 1, 'marco': 1, 'numero': 1, 'como': 1, 'quién': 4, 'quiero': 8, 'necesito': 3, 'comunique': 1, 'oiga': 1, 'yo': 2, 'no': 2, 'sé': 1, 'llamada': 1, 'poner': 1, 'te': 1, 'trató': 2, 'hablé': 1, 'voz': 3, 'dejar': 3, 'quiciera': 1, 'mensaje': 2, 'mesaje': 1, 'buzón': 1, 'servicio': 3, 'alguien': 6, 'mercadotecnia': 1, 'ofimática': 3, 'proporcionar': 1, 'informática': 1, 'área': 11, 'canalizar': 1, 'proveedores': 1, 'podrás': 1, 'revisa': 1, 'nombre': 1, 'servicios': 2, 'mi': 1, 'productos': 1, 'mis': 1, 'ofrecer': 1, 'ofreciendo': 1, 'mire': 2, 'estamos': 1, 'llamó': 1, 'le': 1, 'referencias': 10, 'personas': 2, 'las': 2, 'recursos': 10, 'laboral': 9, 'humanos': 10, 'dan': 2, 'comunicame': 3, 'donde': 2, 'laborales': 4, 'ustedes': 6, 'laboró': 3, 'trabajó': 3, 'comentado': 2, 'habló': 2, 'ha': 2, 'sobre': 3, 'apoyar': 1, 'apoye': 1, 'deuna': 1, 'ver': 2, 'vacante': 3, 'lo': 2, 'sus': 1, 'vacantes': 1, 'dame': 2, 'bh': 1, 'segundo': 2, 'rh': 1, 'hacer': 1, 'jovencita': 1, 'al': 4, 'xannan': 2, 'encargado': 2, 'debo': 1, 'voy': 1, 'técnico': 1, 'soporte': 6, 'jannan': 1, 'area': 1, 'telecomunicaciones': 1, 'estado': 1, 'gerente': 2, 'tesorería': 2, 'tratando': 2, 'localizar': 2, 'ventas': 4, 'licencias': 1, 'contacto': 1, 'compras': 1, 'departamento': 2, 'conde': 1, 'compra': 2, 'respecto': 2, 'software': 2, 'ayudarme': 1, 'comercial': 1, 'asesor': 1, 'vigilancia': 1, 'usted': 1, 'gentil': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"word_docs: \",t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convertir a matriz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(X_, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = copy.deepcopy(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sería tan amable de poderme comunicar con la persona encargada de solicitar un correo electrónico',\n",
       "       'Estoy buscando por favor a de buró de crédito',\n",
       "       'con de contabilidad',\n",
       "       'sería tan amable de comunicarme con la contabilidad por favor ',\n",
       "       'habla de DUMMY sería tan amable de comunicarme a contabilidad'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 1 0 2 0 1 1 0 0 0 1 1 1 0 1 1 1 1]\n",
      " [0 1 1 0 0 0 0 0 1 2 0 0 0 1 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0 2 1 0 0 0 0 1 0 0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_ = vectorizer.fit_transform(X_)\n",
    "print(X_.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amable', 'buró', 'buscando', 'comunicar', 'comunicarme', 'con', 'contabilidad', 'correo', 'crédito', 'de', 'dummy', 'electrónico', 'encargada', 'estoy', 'favor', 'habla', 'la', 'persona', 'poderme', 'por', 'sería', 'solicitar', 'tan', 'un']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amable': 0,\n",
       " 'buró': 1,\n",
       " 'buscando': 2,\n",
       " 'comunicar': 3,\n",
       " 'comunicarme': 4,\n",
       " 'con': 5,\n",
       " 'contabilidad': 6,\n",
       " 'correo': 7,\n",
       " 'crédito': 8,\n",
       " 'de': 9,\n",
       " 'dummy': 10,\n",
       " 'electrónico': 11,\n",
       " 'encargada': 12,\n",
       " 'estoy': 13,\n",
       " 'favor': 14,\n",
       " 'habla': 15,\n",
       " 'la': 16,\n",
       " 'persona': 17,\n",
       " 'poderme': 18,\n",
       " 'por': 19,\n",
       " 'sería': 20,\n",
       " 'solicitar': 21,\n",
       " 'tan': 22,\n",
       " 'un': 23}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for v, k in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work flow completo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sería tan amable de poderme comunicar con la persona encargada de solicitar un correo electrónico',\n",
       "       'Estoy buscando por favor a de buró de crédito',\n",
       "       'con de contabilidad',\n",
       "       'sería tan amable de comunicarme con la contabilidad por favor ',\n",
       "       'habla de DUMMY sería tan amable de comunicarme a contabilidad'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de bolsa de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenar todas las oraciones\n",
    "join_sentence = ' '\n",
    "X_ = join_sentence.join(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sería tan amable de poderme comunicar con la persona encargada de solicitar un correo electrónico Estoy buscando por favor a de buró de crédito con de contabilidad sería tan amable de comunicarme con '"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizar el texto\n",
    "X_ = normalizar_string(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seria tan amable de poderme comunicar con la persona encargada de solicitar un correo electronico estoy buscando por favor a de buro de credito con de contabilidad seria tan amable de comunicarme con '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizar\n",
    "words = nltk.word_tokenize(X_)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seria', 'tan', 'amable', 'de', 'poderme']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminar palabras vacias\n",
    "\n",
    "words_ = []\n",
    "[words_.append(w) for w in words if not w in spanish_stopwords]\n",
    "words = words_\n",
    "del words_\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['senorit', 'ver', 'ofimat', 'oficin', 'mercadotecni', 'cond', 'sistem', 'ahi', 'pud', 'quier', 'vent', 'habl', 'senor', 'nuev', 'preci', 'contador', 'dan', 'apoy', 'human', 'cominic', 'pen', 'curs', 'comunicam', 'comun', 'tal', 'molesti', 'oig', 'favor', 'corpor', 'jann', 'jovencit', 'xann', 'gustari', 'telecomun', 'comunic', 'trabaj', 'deb', 'vacant', 'compr', 'si', 'labor', 'buzon', 'amabl', 'comercial', 'mesaj', 'intent', 'pued', 'softwar', 'corre', 'proveedor', 'usted', 'arrend', 'ingenier', 'dej', 'product', 'queri', 'seri', 'dam', 'algui', 'localiz', 'ayud', 'pur', 'recurs', 'tesoreri', 'respect', 'are', 'extension', 'gentil', 'mir', 'comuniqu', 'encuentr', 'direccion', 'onda', 'hac', 'solicit', 'llam', 'quis', 'prodri', 'disculp', 'servici', 'da', 'deun', 'graci', 'rh', 'asesor', 'bur', 'general', 'numer', 'voy', 'revis', 'quic', 'mensaj', 'perdon', 'referent', 'conect', 'gerent', 'oye', 'voz', 'trat', 'credit', 'contabil', 'celul', 'encarg', 'informat', 'financier', 'podri', 'ofrec', 'dummy', 'pod', 'transfer', 'necesit', 'molestandot', 'informacion', 'coment', 'vigil', 'bh', 'departament', 'person', 'empres', 'canaliz', 'licenci', 'contact', 'podr', 'tecnic', 'segund', 'soport', 'disculpam', 'proporcion', 'tendr', 'nombr', 'laboral', 'busc', 'pon', 'electron', 'iban', 'marc']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('spanish', ignore_stopwords=True)\n",
    "ignore_words = ['?']\n",
    "\n",
    "#stemizar\n",
    "words = [stemmer.stem(w) for w in words if w not in ignore_words]\n",
    "words = list(set(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_counts:  OrderedDict([('senorit', 1), ('ver', 1), ('ofimat', 1), ('oficin', 1), ('mercadotecni', 1), ('cond', 1), ('sistem', 1), ('ahi', 1), ('pud', 1), ('quier', 1), ('vent', 1), ('habl', 1), ('senor', 1), ('nuev', 1), ('preci', 1), ('contador', 1), ('dan', 1), ('apoy', 1), ('human', 1), ('cominic', 1), ('pen', 1), ('curs', 1), ('comunicam', 1), ('comun', 1), ('tal', 1), ('molesti', 1), ('oig', 1), ('favor', 1), ('corpor', 1), ('jann', 1), ('jovencit', 1), ('xann', 1), ('gustari', 1), ('telecomun', 1), ('comunic', 1), ('trabaj', 1), ('deb', 1), ('vacant', 1), ('compr', 1), ('si', 1), ('labor', 1), ('buzon', 1), ('amabl', 1), ('comercial', 1), ('mesaj', 1), ('intent', 1), ('pued', 1), ('softwar', 1), ('corre', 1), ('proveedor', 1), ('usted', 1), ('arrend', 1), ('ingenier', 1), ('dej', 1), ('product', 1), ('queri', 1), ('seri', 1), ('dam', 1), ('algui', 1), ('localiz', 1), ('ayud', 1), ('pur', 1), ('recurs', 1), ('tesoreri', 1), ('respect', 1), ('are', 1), ('extension', 1), ('gentil', 1), ('mir', 1), ('comuniqu', 1), ('encuentr', 1), ('direccion', 1), ('onda', 1), ('hac', 1), ('solicit', 1), ('llam', 1), ('quis', 1), ('prodri', 1), ('disculp', 1), ('servici', 1), ('da', 1), ('deun', 1), ('graci', 1), ('rh', 1), ('asesor', 1), ('bur', 1), ('general', 1), ('numer', 1), ('voy', 1), ('revis', 1), ('quic', 1), ('mensaj', 1), ('perdon', 1), ('referent', 1), ('conect', 1), ('gerent', 1), ('oye', 1), ('voz', 1), ('trat', 1), ('credit', 1), ('contabil', 1), ('celul', 1), ('encarg', 1), ('informat', 1), ('financier', 1), ('podri', 1), ('ofrec', 1), ('dummy', 1), ('pod', 1), ('transfer', 1), ('necesit', 1), ('molestandot', 1), ('informacion', 1), ('coment', 1), ('vigil', 1), ('bh', 1), ('departament', 1), ('person', 1), ('empres', 1), ('canaliz', 1), ('licenci', 1), ('contact', 1), ('podr', 1), ('tecnic', 1), ('segund', 1), ('soport', 1), ('disculpam', 1), ('proporcion', 1), ('tendr', 1), ('nombr', 1), ('laboral', 1), ('busc', 1), ('pon', 1), ('electron', 1), ('iban', 1), ('marc', 1)])\n"
     ]
    }
   ],
   "source": [
    "# summarize what was learned\n",
    "print(\"word_counts: \", t.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_count:  136\n"
     ]
    }
   ],
   "source": [
    "print(\"document_count: \",t.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index:  {'senorit': 1, 'ver': 2, 'ofimat': 3, 'oficin': 4, 'mercadotecni': 5, 'cond': 6, 'sistem': 7, 'ahi': 8, 'pud': 9, 'quier': 10, 'vent': 11, 'habl': 12, 'senor': 13, 'nuev': 14, 'preci': 15, 'contador': 16, 'dan': 17, 'apoy': 18, 'human': 19, 'cominic': 20, 'pen': 21, 'curs': 22, 'comunicam': 23, 'comun': 24, 'tal': 25, 'molesti': 26, 'oig': 27, 'favor': 28, 'corpor': 29, 'jann': 30, 'jovencit': 31, 'xann': 32, 'gustari': 33, 'telecomun': 34, 'comunic': 35, 'trabaj': 36, 'deb': 37, 'vacant': 38, 'compr': 39, 'si': 40, 'labor': 41, 'buzon': 42, 'amabl': 43, 'comercial': 44, 'mesaj': 45, 'intent': 46, 'pued': 47, 'softwar': 48, 'corre': 49, 'proveedor': 50, 'usted': 51, 'arrend': 52, 'ingenier': 53, 'dej': 54, 'product': 55, 'queri': 56, 'seri': 57, 'dam': 58, 'algui': 59, 'localiz': 60, 'ayud': 61, 'pur': 62, 'recurs': 63, 'tesoreri': 64, 'respect': 65, 'are': 66, 'extension': 67, 'gentil': 68, 'mir': 69, 'comuniqu': 70, 'encuentr': 71, 'direccion': 72, 'onda': 73, 'hac': 74, 'solicit': 75, 'llam': 76, 'quis': 77, 'prodri': 78, 'disculp': 79, 'servici': 80, 'da': 81, 'deun': 82, 'graci': 83, 'rh': 84, 'asesor': 85, 'bur': 86, 'general': 87, 'numer': 88, 'voy': 89, 'revis': 90, 'quic': 91, 'mensaj': 92, 'perdon': 93, 'referent': 94, 'conect': 95, 'gerent': 96, 'oye': 97, 'voz': 98, 'trat': 99, 'credit': 100, 'contabil': 101, 'celul': 102, 'encarg': 103, 'informat': 104, 'financier': 105, 'podri': 106, 'ofrec': 107, 'dummy': 108, 'pod': 109, 'transfer': 110, 'necesit': 111, 'molestandot': 112, 'informacion': 113, 'coment': 114, 'vigil': 115, 'bh': 116, 'departament': 117, 'person': 118, 'empres': 119, 'canaliz': 120, 'licenci': 121, 'contact': 122, 'podr': 123, 'tecnic': 124, 'segund': 125, 'soport': 126, 'disculpam': 127, 'proporcion': 128, 'tendr': 129, 'nombr': 130, 'laboral': 131, 'busc': 132, 'pon': 133, 'electron': 134, 'iban': 135, 'marc': 136}\n"
     ]
    }
   ],
   "source": [
    "print(\"word_index: \", t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_clean(sentence):\n",
    "    sentence = normalizar_string(sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words_ = []\n",
    "    [words_.append(w) for w in words if not w in spanish_stopwords]\n",
    "    words = words_\n",
    "    del words_\n",
    "\n",
    "    #stemizar, pasar a minúscula y eliminar duplicadas \n",
    "    words = [stemmer.stem(w) for w in words if w not in ignore_words]\n",
    "    \n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hol pued comunic'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_clean(\"hola me puedes comunicar con \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sería tan amable de poderme comunicar con la persona encargada de solicitar un correo electrónico'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = [sentence_clean(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seri amabl pod comunic person encarg solicit corre electron',\n",
       " 'busc favor bur credit',\n",
       " 'contabil',\n",
       " 'seri amabl comunic contabil favor',\n",
       " 'habl dummy seri amabl comunic contabil',\n",
       " 'podri comunic contador general favor',\n",
       " 'gustari pod contact person encarg arrend pur financier',\n",
       " 'favor informacion curs',\n",
       " 'tal comun direccion general favor',\n",
       " 'person licenci informat',\n",
       " 'disculpam molesti da pen pued comunic',\n",
       " 'disculp podri comunic extension',\n",
       " 'busc',\n",
       " 'comunic extension soport',\n",
       " 'contador',\n",
       " '']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode documents\n",
    "#mode = c(\"binary\", \"count\", \"tfidf\",\"freq\")\n",
    "encoded_docs = t.texts_to_matrix(X_, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disculpam molesti da pen pued comunic\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "21\n",
      "26\n",
      "35\n",
      "47\n",
      "81\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "print(X_[10])\n",
    "print(encoded_docs[10])\n",
    "for i,w in enumerate (encoded_docs[10]):\n",
    "    if w == 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### otra opción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_v = vectorizer.fit_transform(X_)\n",
    "print(X_v.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'discúlpame la molestia  me da pena  pero me puedes comunicar con una'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_v.toarray()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  19\n",
      "1 :  31\n",
      "1 :  40\n",
      "1 :  75\n",
      "1 :  86\n",
      "1 :  99\n"
     ]
    }
   ],
   "source": [
    "for i,w in enumerate (X_v.toarray()[10]):\n",
    "    if w > 0:\n",
    "        print(w,': ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ahi': 0,\n",
       " 'algui': 1,\n",
       " 'amabl': 2,\n",
       " 'apoy': 3,\n",
       " 'are': 4,\n",
       " 'arrend': 5,\n",
       " 'asesor': 6,\n",
       " 'ayud': 7,\n",
       " 'bh': 8,\n",
       " 'bur': 9,\n",
       " 'busc': 10,\n",
       " 'buzon': 11,\n",
       " 'canaliz': 12,\n",
       " 'celul': 13,\n",
       " 'coment': 14,\n",
       " 'comercial': 15,\n",
       " 'cominic': 16,\n",
       " 'compr': 17,\n",
       " 'comun': 18,\n",
       " 'comunic': 19,\n",
       " 'comunicam': 20,\n",
       " 'comuniqu': 21,\n",
       " 'cond': 22,\n",
       " 'conect': 23,\n",
       " 'contabil': 24,\n",
       " 'contact': 25,\n",
       " 'contador': 26,\n",
       " 'corpor': 27,\n",
       " 'corre': 28,\n",
       " 'credit': 29,\n",
       " 'curs': 30,\n",
       " 'da': 31,\n",
       " 'dam': 32,\n",
       " 'dan': 33,\n",
       " 'deb': 34,\n",
       " 'dej': 35,\n",
       " 'departament': 36,\n",
       " 'deun': 37,\n",
       " 'direccion': 38,\n",
       " 'disculp': 39,\n",
       " 'disculpam': 40,\n",
       " 'dummy': 41,\n",
       " 'electron': 42,\n",
       " 'empres': 43,\n",
       " 'encarg': 44,\n",
       " 'encuentr': 45,\n",
       " 'extension': 46,\n",
       " 'favor': 47,\n",
       " 'financier': 48,\n",
       " 'general': 49,\n",
       " 'gentil': 50,\n",
       " 'gerent': 51,\n",
       " 'graci': 52,\n",
       " 'gustari': 53,\n",
       " 'habl': 54,\n",
       " 'hac': 55,\n",
       " 'human': 56,\n",
       " 'iban': 57,\n",
       " 'informacion': 58,\n",
       " 'informat': 59,\n",
       " 'ingenier': 60,\n",
       " 'intent': 61,\n",
       " 'jann': 62,\n",
       " 'jovencit': 63,\n",
       " 'labor': 64,\n",
       " 'laboral': 65,\n",
       " 'licenci': 66,\n",
       " 'llam': 67,\n",
       " 'localiz': 68,\n",
       " 'marc': 69,\n",
       " 'mensaj': 70,\n",
       " 'mercadotecni': 71,\n",
       " 'mesaj': 72,\n",
       " 'mir': 73,\n",
       " 'molestandot': 74,\n",
       " 'molesti': 75,\n",
       " 'necesit': 76,\n",
       " 'nombr': 77,\n",
       " 'nuev': 78,\n",
       " 'numer': 79,\n",
       " 'oficin': 80,\n",
       " 'ofimat': 81,\n",
       " 'ofrec': 82,\n",
       " 'oig': 83,\n",
       " 'onda': 84,\n",
       " 'oye': 85,\n",
       " 'pen': 86,\n",
       " 'perdon': 87,\n",
       " 'person': 88,\n",
       " 'pod': 89,\n",
       " 'podr': 90,\n",
       " 'podri': 91,\n",
       " 'pon': 92,\n",
       " 'preci': 93,\n",
       " 'prodri': 94,\n",
       " 'product': 95,\n",
       " 'proporcion': 96,\n",
       " 'proveedor': 97,\n",
       " 'pud': 98,\n",
       " 'pued': 99,\n",
       " 'pur': 100,\n",
       " 'queri': 101,\n",
       " 'quic': 102,\n",
       " 'quier': 103,\n",
       " 'quis': 104,\n",
       " 'recurs': 105,\n",
       " 'referent': 106,\n",
       " 'respect': 107,\n",
       " 'revis': 108,\n",
       " 'rh': 109,\n",
       " 'segund': 110,\n",
       " 'senor': 111,\n",
       " 'senorit': 112,\n",
       " 'seri': 113,\n",
       " 'servici': 114,\n",
       " 'si': 115,\n",
       " 'sistem': 116,\n",
       " 'softwar': 117,\n",
       " 'solicit': 118,\n",
       " 'soport': 119,\n",
       " 'tal': 120,\n",
       " 'tecnic': 121,\n",
       " 'telecomun': 122,\n",
       " 'tendr': 123,\n",
       " 'tesoreri': 124,\n",
       " 'trabaj': 125,\n",
       " 'transfer': 126,\n",
       " 'trat': 127,\n",
       " 'usted': 128,\n",
       " 'vacant': 129,\n",
       " 'vent': 130,\n",
       " 'ver': 131,\n",
       " 'vigil': 132,\n",
       " 'voy': 133,\n",
       " 'voz': 134,\n",
       " 'xann': 135}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for v, k in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#almacenar modelo de lenguaje\n",
    "f = open('words.pckl', 'wb')\n",
    "pickle.dump(words, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "#almacenar las stop_words\n",
    "f = open('spanish_stopwords.pckl', 'wb')\n",
    "pickle.dump(spanish_stopwords, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "#almacenar el modelo de BoW\n",
    "f = open('CountVectorizer.pckl', 'wb')\n",
    "pickle.dump(vectorizer, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "#almacenar el modelo de BoW\n",
    "f = open('keras_vec.pckl', 'wb')\n",
    "pickle.dump(t, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_final = pd.DataFrame(encoded_docs)\n",
    "\n",
    "dataset_final = pd.DataFrame(X_v.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 138)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_final['class'] = Y\n",
    "dataset_final['class_cat'] = df['class']\n",
    "dataset_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 138)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping  duplicte values \n",
    "dataset_final.drop_duplicates(keep = 'first', inplace = True) #subset =\"First Name\"\n",
    "dataset_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.to_csv(\"WoF.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Negación: la gran mayoría de bots no entienden la negación que puede aparecer en cualquier frase por la sencilla razón de que su construcción se basa en keywords. Esto complica e impide que los usuarios puedan utilizar en sus conversaciones frases tan sencillas como “Quiero una pizza campesina, pero sin champiñones”. A esto se suman casos más complejos y que también pueden darse en una conversación. Pensamos en casos como:\n",
    "\n",
    "    \n",
    "    - Quiero una pizza barbacoa sin cerdo (solo una parte de la frase es negativa).\n",
    "    - No queremos bebidas (toda la frase es negativa).\n",
    "    - No estoy seguro… tomaré una cerveza (no se niega la cuestión principal).\n",
    "\n",
    "\n",
    "- Coordinación: es uno de los recursos más utilizados en las conversaciones entre humanos. La mayoría de las plataformas no comprenden oraciones en las que los elementos estén conectados con un nexo. Nuestro conocimiento lingüístico es capaz de resolver ese problema:\n",
    "\n",
    "    - [Quiero una pizza Hawaiana] y [mi mujer quiere una Margarita] (Dos cláusulas principales).\n",
    "    \n",
    "    - Quiero una Hawaiana [con [extra de queso] y [cebolla]] (una modificación añadiendo dos ingredientes).\n",
    "    \n",
    "    - Pediré [[una Hawaiana [con [extra de queso] y [cebolla]]] y [una Margarita] (dos pizzas y además la primera con dos ingredientes).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Conexión entre diferentes frases: la mayoría de los chatbots han sido diseñados siguiendo el modelo de árbol de decisión, por lo que un usuario no puede modificar sus peticiones y se ve obligado a volver a empezar el proceso de compra. Como solución nosotros proponemos usar conectores tal y como te explicamos en el siguiente ejemplo:\n",
    "\n",
    "    - Quiero una margarita con cebolla… Además, con extra de queso (Añades información en la primera frase: añades un ingrediente).\n",
    "    \n",
    "    - Quiero una Hawaiana con extra de piña. Aunque la prefiero sin jamón (añade información antes y después)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "### N-gramas\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_v = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.46979138557992045\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.46979138557992045\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 8)\t0.38408524091481483\n"
     ]
    }
   ],
   "source": [
    "print(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seri amabl pod comunic person encarg solicit corre electron',\n",
       " 'busc favor bur credit',\n",
       " 'contabil',\n",
       " 'seri amabl comunic contabil favor',\n",
       " 'habl dummy seri amabl comunic contabil',\n",
       " 'podri comunic contador general favor',\n",
       " 'gustari pod contact person encarg arrend pur financier',\n",
       " 'favor informacion curs',\n",
       " 'tal comun direccion general favor',\n",
       " 'person licenci informat',\n",
       " 'disculpam molesti da pen pued comunic',\n",
       " 'disculp podri comunic extension',\n",
       " 'busc',\n",
       " 'comunic extension soport',\n",
       " 'contador',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'disculpam prodri comunic',\n",
       " 'disculpam encuentr',\n",
       " 'empres dummy disculp busc',\n",
       " 'corpor dummy',\n",
       " 'disculp',\n",
       " 'disculp oficin',\n",
       " 'favor',\n",
       " 'disculp podr conect extension',\n",
       " 'disculp podri comunic senor',\n",
       " 'disculp pued comunic senor',\n",
       " 'disculp pued transfer extension senorit',\n",
       " 'disculp favor',\n",
       " 'disculp si pued comunic',\n",
       " 'seri amabl comunic',\n",
       " 'encuentr ahi',\n",
       " 'busc',\n",
       " 'dummy podri comunic',\n",
       " 'seri amabl comunic',\n",
       " 'habl dummy intent comunic',\n",
       " 'habl dummy intent comunic',\n",
       " 'habl senor intent comunic senorit',\n",
       " 'intent comunic senor',\n",
       " 'extension senorit',\n",
       " 'comun',\n",
       " 'comun sistem',\n",
       " 'comun favor',\n",
       " 'comun',\n",
       " 'comun',\n",
       " 'comun',\n",
       " 'comun',\n",
       " 'iban comunic',\n",
       " 'pod comunic',\n",
       " 'podri cominic',\n",
       " 'podri comunic',\n",
       " 'podri comunic',\n",
       " 'podri comunic senor',\n",
       " 'podri comunic senor',\n",
       " 'podri comunic',\n",
       " 'podri comunic',\n",
       " 'podri comunic preci',\n",
       " 'podri comunic',\n",
       " 'podri comunic',\n",
       " 'podri comunic',\n",
       " 'podri comunic si graci',\n",
       " 'podri apoy comunic',\n",
       " 'oye disculp busc licenci',\n",
       " 'podri comunic',\n",
       " 'podri comunic',\n",
       " 'podri comunic',\n",
       " 'podri comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued ayud comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'pued comunic',\n",
       " 'disculp pued comunic',\n",
       " 'favor',\n",
       " 'pen busc',\n",
       " 'podri comunic favor',\n",
       " 'nuev molestandot',\n",
       " 'disculp',\n",
       " 'oye pued comunic',\n",
       " 'perdon quis habl',\n",
       " 'tendr numer celul extension',\n",
       " 'tendr numer celul extension',\n",
       " 'tendr numer extension',\n",
       " 'podr comunic favor',\n",
       " 'podr habl',\n",
       " 'podri favor comunic',\n",
       " 'podri comunic favor',\n",
       " 'favor',\n",
       " 'pud comunic',\n",
       " 'onda favor ingenier',\n",
       " 'tal podri comunic',\n",
       " 'queri comunic favor',\n",
       " 'marc dej numer referent',\n",
       " 'quier habl',\n",
       " 'encuentr oficin',\n",
       " 'disculp molesti necesit',\n",
       " 'disculp encuentr',\n",
       " 'podri comunic favor',\n",
       " 'comuniqu ingenier',\n",
       " 'podri comunic senor',\n",
       " 'pued comunic senor',\n",
       " 'pued comunic',\n",
       " 'oig necesit habl senor',\n",
       " 'quis habl senor',\n",
       " 'seri amabl comunic extension',\n",
       " 'seri amabl comunic',\n",
       " 'si podri comunic',\n",
       " 'si podri comunic',\n",
       " 'si pued comunic',\n",
       " 'si pued pon llam favor',\n",
       " 'si necesit habl favor',\n",
       " 'si trat comunic senor',\n",
       " 'si trat comunic senor',\n",
       " 'habl comunic',\n",
       " 'quic dej mensaj voz',\n",
       " 'quier dej mensaj voz',\n",
       " 'quier dej mesaj',\n",
       " 'servici buzon voz',\n",
       " 'algui mercadotecni',\n",
       " 'pued comunic ofimat',\n",
       " 'podri comunic ofimat',\n",
       " 'pued proporcion numer ofimat',\n",
       " 'gustari habl person encarg are informat',\n",
       " 'disculp si podr canaliz person revis corre servici proveedor',\n",
       " 'nombr disculp pued habl ofrec product servici',\n",
       " 'mir llam empres dummy ofrec servici',\n",
       " 'pued comunic person referent',\n",
       " 'pued comunic person referent',\n",
       " 'comunicam recurs human dan referent laboral',\n",
       " 'comunicam recurs human dan referent laboral',\n",
       " 'comunicam recurs human referent laboral',\n",
       " 'recurs human solicit referent laboral',\n",
       " 'recurs human solicit referent laboral',\n",
       " 'disculp quier solicit referent laboral',\n",
       " 'disculp quier solicit referent laboral',\n",
       " 'disculp busc referent person labor usted',\n",
       " 'disculp busc referent person trabaj usted',\n",
       " 'disculp busc referent person trabaj usted',\n",
       " 'disculp busc referent person labor usted',\n",
       " 'habl referent person coment labor usted',\n",
       " 'habl referent person coment trabaj usted',\n",
       " 'disculp solicit referent laboral recurs human',\n",
       " 'referent laboral',\n",
       " 'referent laboral',\n",
       " 'disculp podri comunic algui apoy referent laboral',\n",
       " 'disculp podri comunic algui apoy referent laboral',\n",
       " 'podri comunic recurs human referent laboral',\n",
       " 'quier ver deun vacant',\n",
       " 'quier ver vacant',\n",
       " 'quier informacion vacant',\n",
       " 'habl solicit informacion vacant',\n",
       " 'disculp molesti pued comunic recurs human',\n",
       " 'pued comunic algui bh favor dam segund',\n",
       " 'pued comunic algui rh favor dam segund',\n",
       " 'jovencit pued hac favor comunic are recurs human',\n",
       " 'disculp busc recurs human',\n",
       " 'queri comunic servici xann favor',\n",
       " 'are sistem',\n",
       " 'are sistem',\n",
       " 'are sistem favor',\n",
       " 'tal quis habl encarg are sistem',\n",
       " 'queri habl encarg are sistem favor',\n",
       " 'deb comunic are sistem favor',\n",
       " 'voy comunic soport tecnic favor',\n",
       " 'podri comunic soport',\n",
       " 'comun soport favor graci',\n",
       " 'pued comunic soport',\n",
       " 'podri comunic are soport jann',\n",
       " 'podri comunic are soport xann',\n",
       " 'pued comunic are telecomun',\n",
       " 'disculp trat localiz gerent tesoreri',\n",
       " 'disculp trat localiz gerent tesoreri',\n",
       " 'algui vent',\n",
       " 'licenci contact vent',\n",
       " 'podri comunic are compr favor',\n",
       " 'pued comunic cond departament vent',\n",
       " 'pued comunic departament vent',\n",
       " 'pued ayud respect compr softwar',\n",
       " 'pued ayud respect compr softwar',\n",
       " 'mir busc asesor comercial',\n",
       " 'favor seri usted gentil comunic vigil']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
